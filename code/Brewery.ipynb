{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and spark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set SPARK_HOME\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:/Spark/spark-3.5.0-bin-hadoop3\"\n",
    "\n",
    "# Set PYTHONPATH\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", \"\")\n",
    "if spark_home:\n",
    "    python_path = os.path.join(spark_home, \"python\")\n",
    "    py4j_zip = os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9.7-src.zip\")\n",
    "    os.environ[\"PYTHONPATH\"] = f\"{python_path};{py4j_zip};{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Set PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\"\n",
    "print(os.environ.get('PYSPARK_PYTHON'))\n",
    "\n",
    "# Set PATH\n",
    "if spark_home:\n",
    "    os.environ[\"PATH\"] = f\"{python_path};{os.path.join(spark_home, 'bin')};{os.environ.get('PATH', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import gc\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20fbffaa520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"spark\").config(\"spark.driver.memory\", \"24g\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "# spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(r\"c:\\Users\\Vincenzo\\PROJECTS\\DDAM_data\\brewery\\brewery_data_complete_extended.parquet\")\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batch_ID: long (nullable = true)\n",
      " |-- Brew_Date: string (nullable = true)\n",
      " |-- Beer_Style: string (nullable = true)\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Fermentation_Time: long (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- pH_Level: double (nullable = true)\n",
      " |-- Gravity: double (nullable = true)\n",
      " |-- Alcohol_Content: double (nullable = true)\n",
      " |-- Bitterness: long (nullable = true)\n",
      " |-- Color: long (nullable = true)\n",
      " |-- Ingredient_Ratio: string (nullable = true)\n",
      " |-- Volume_Produced: long (nullable = true)\n",
      " |-- Total_Sales: double (nullable = true)\n",
      " |-- Quality_Score: double (nullable = true)\n",
      " |-- Brewhouse_Efficiency: double (nullable = true)\n",
      " |-- Loss_During_Brewing: double (nullable = true)\n",
      " |-- Loss_During_Fermentation: double (nullable = true)\n",
      " |-- Loss_During_Bottling_Kegging: double (nullable = true)\n",
      "\n",
      "Dimension of the Dataframe is: (10000000, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "|Batch_ID|          Brew_Date|Beer_Style| SKU|    Location|Fermentation_Time|       Temperature|          pH_Level|           Gravity|  Alcohol_Content|Bitterness|Color|Ingredient_Ratio|Volume_Produced|       Total_Sales|    Quality_Score|Brewhouse_Efficiency|Loss_During_Brewing|Loss_During_Fermentation|Loss_During_Bottling_Kegging|\n",
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "| 7870796|2020-01-01 00:00:19|Wheat Beer|Kegs|  Whitefield|               16|24.204250857069876|5.2898454476095615| 1.039504126730198|5.370842159553436|        20|    5|     1:0.32:0.16|           4666|2664.7593448382822| 8.57701633109399|   89.19588216376087| 4.1049876591878345|      3.2354851724654683|           4.663204448186049|\n",
      "| 9810411|2020-01-01 00:00:31|      Sour|Kegs|  Whitefield|               13|18.086762947259544| 5.275643382756193|1.0598189516987164|5.096053082797625|        36|   14|     1:0.39:0.24|            832|  9758.80106247132|7.420540752553908|    72.4809153900275|  2.676528095392112|      4.2461292104108574|            2.04435836917023|\n",
      "| 2623342|2020-01-01 00:00:40|Wheat Beer|Kegs| Malleswaram|               12|15.539332669116469|4.7780156232459765|  1.03747570954872|4.824737120959184|        30|   10|     1:0.35:0.16|           2115|11721.087016274963|8.451364886803127|   86.32214396020584|  3.299893625514981|       3.109440467362847|           3.033879837876281|\n",
      "| 8114651|2020-01-01 00:01:37|       Ale|Kegs| Rajajinagar|               17| 16.41848910394318| 5.345260585546188|1.0524314251694946|5.509243080797997|        48|   18|     1:0.35:0.15|           3173|12050.177463190275|9.671859404043175|   83.09494037181545|  2.136055116262562|       4.634254174098425|          1.4898890677148424|\n",
      "| 4579587|2020-01-01 00:01:43|     Stout|Cans|Marathahalli|               18| 19.14490765433852|  4.86185374113861|1.0542961149482333|5.133624684263243|        57|   13|     1:0.46:0.11|           4449|5515.0774647529615|7.895333676172065|   88.62583302052388|  4.491723843594972|      2.1833886016455497|          2.9906302188791485|\n",
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "print(f'Dimension of the Dataframe is: {(rows,cols)}')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and df assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1001591\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = sample.toPandas().describe()\n",
    "rounded_describe = np.round(summary_stats, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch_ID</th>\n",
       "      <th>Fermentation_Time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>pH_Level</th>\n",
       "      <th>Gravity</th>\n",
       "      <th>Alcohol_Content</th>\n",
       "      <th>Bitterness</th>\n",
       "      <th>Color</th>\n",
       "      <th>Volume_Produced</th>\n",
       "      <th>Total_Sales</th>\n",
       "      <th>Quality_Score</th>\n",
       "      <th>Brewhouse_Efficiency</th>\n",
       "      <th>Loss_During_Brewing</th>\n",
       "      <th>Loss_During_Fermentation</th>\n",
       "      <th>Loss_During_Bottling_Kegging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "      <td>1001591.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5004909.19</td>\n",
       "      <td>14.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>5.25</td>\n",
       "      <td>39.52</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2749.48</td>\n",
       "      <td>10491.48</td>\n",
       "      <td>8.00</td>\n",
       "      <td>79.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2887164.29</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>11.54</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1298.31</td>\n",
       "      <td>5486.58</td>\n",
       "      <td>1.15</td>\n",
       "      <td>5.78</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.03</td>\n",
       "      <td>4.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>500.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2502862.50</td>\n",
       "      <td>12.00</td>\n",
       "      <td>17.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.88</td>\n",
       "      <td>30.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1624.00</td>\n",
       "      <td>5747.89</td>\n",
       "      <td>7.00</td>\n",
       "      <td>74.98</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5008192.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>5.25</td>\n",
       "      <td>40.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2751.00</td>\n",
       "      <td>10486.75</td>\n",
       "      <td>8.00</td>\n",
       "      <td>79.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7506431.50</td>\n",
       "      <td>17.00</td>\n",
       "      <td>22.51</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.63</td>\n",
       "      <td>50.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3874.00</td>\n",
       "      <td>15240.40</td>\n",
       "      <td>9.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999993.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.08</td>\n",
       "      <td>6.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>4999.00</td>\n",
       "      <td>19999.97</td>\n",
       "      <td>10.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Batch_ID  Fermentation_Time  Temperature    pH_Level     Gravity  \\\n",
       "count  1001591.00         1001591.00   1001591.00  1001591.00  1001591.00   \n",
       "mean   5004909.19              14.50        20.00        5.00        1.06   \n",
       "std    2887164.29               2.87         2.89        0.29        0.01   \n",
       "min          5.00              10.00        15.00        4.50        1.03   \n",
       "25%    2502862.50              12.00        17.50        4.75        1.04   \n",
       "50%    5008192.00              15.00        20.00        5.00        1.06   \n",
       "75%    7506431.50              17.00        22.51        5.25        1.07   \n",
       "max    9999993.00              19.00        25.00        5.50        1.08   \n",
       "\n",
       "       Alcohol_Content  Bitterness       Color  Volume_Produced  Total_Sales  \\\n",
       "count       1001591.00  1001591.00  1001591.00       1001591.00   1001591.00   \n",
       "mean              5.25       39.52       12.00          2749.48     10491.48   \n",
       "std               0.43       11.54        4.32          1298.31      5486.58   \n",
       "min               4.50       20.00        5.00           500.00      1000.00   \n",
       "25%               4.88       30.00        8.00          1624.00      5747.89   \n",
       "50%               5.25       40.00       12.00          2751.00     10486.75   \n",
       "75%               5.63       50.00       16.00          3874.00     15240.40   \n",
       "max               6.00       59.00       19.00          4999.00     19999.97   \n",
       "\n",
       "       Quality_Score  Brewhouse_Efficiency  Loss_During_Brewing  \\\n",
       "count     1001591.00            1001591.00           1001591.00   \n",
       "mean            8.00                 79.99                 3.00   \n",
       "std             1.15                  5.78                 1.15   \n",
       "min             6.00                 70.00                 1.00   \n",
       "25%             7.00                 74.98                 2.00   \n",
       "50%             8.00                 79.99                 3.00   \n",
       "75%             9.00                 85.00                 4.00   \n",
       "max            10.00                 90.00                 5.00   \n",
       "\n",
       "       Loss_During_Fermentation  Loss_During_Bottling_Kegging  \n",
       "count                1001591.00                    1001591.00  \n",
       "mean                       3.00                          3.00  \n",
       "std                        1.16                          1.15  \n",
       "min                        1.00                          1.00  \n",
       "25%                        2.00                          2.00  \n",
       "50%                        3.00                          3.00  \n",
       "75%                        4.00                          4.00  \n",
       "max                        5.00                          5.00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempted memory managment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# sample.unpersist()\n",
    "# del summary_stats\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Split the ratio_column into three parts based on the colon delimiter\n",
    "split_col = split(col(\"Ingredient_Ratio\"), \":\")\n",
    "\n",
    "# Create three separate columns for water, grains, and hops\n",
    "df = df.withColumn(\"water\", split_col.getItem(0).cast(\"double\"))\n",
    "df = df.withColumn(\"grains\", split_col.getItem(1).cast(\"double\"))\n",
    "df = df.withColumn(\"hops\", split_col.getItem(2).cast(\"double\"))\n",
    "df = df.drop(\"Ingredient_Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new column with the USD per liter ratio and brewing efficiency\n",
    "df = df.withColumn(\"USD_per_Liter\", F.col(\"Total_Sales\") / F.col(\"Volume_Produced\").cast(\"double\"))\n",
    "df = df.withColumn(\"Brewing_efficency\", F.col(\"Brewhouse_Efficiency\") / F.col(\"Volume_Produced\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['Batch_ID', 'Fermentation_Time', 'Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging', 'water', 'grains', 'hops', 'USD_per_Liter', 'Brewing_efficency']\n",
      "Categorical columns: ['Brew_Date', 'Beer_Style', 'SKU', 'Location']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Identify numerical and non-numerical columns\n",
    "numerical_cols = [col_name for col_name, data_type in df.dtypes if data_type != 'string']\n",
    "categorical_cols = [col_name for col_name, data_type in df.dtypes if data_type == 'string']\n",
    "\n",
    "print(f'Numerical columns: {numerical_cols}')\n",
    "print(f'Categorical columns: {categorical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Batch_ID - 10000000 distinct values\n",
      "- Fermentation_Time - 10 distinct values\n",
      "- Temperature - 10000000 distinct values\n",
      "- pH_Level - 10000000 distinct values\n",
      "- Gravity - 10000000 distinct values\n",
      "- Alcohol_Content - 10000000 distinct values\n",
      "- Bitterness - 40 distinct values\n",
      "- Color - 15 distinct values\n",
      "- Volume_Produced - 4500 distinct values\n",
      "- Total_Sales - 10000000 distinct values\n",
      "- Quality_Score - 10000000 distinct values\n",
      "- Brewhouse_Efficiency - 10000000 distinct values\n",
      "- Loss_During_Brewing - 10000000 distinct values\n",
      "- Loss_During_Fermentation - 10000000 distinct values\n",
      "- Loss_During_Bottling_Kegging - 10000000 distinct values\n",
      "- water - 1 distinct values\n",
      "- grains - 31 distinct values\n",
      "- hops - 21 distinct values\n",
      "- USD_per_Liter - 10000000 distinct values\n",
      "- Brewing_efficency - 10000000 distinct values\n"
     ]
    }
   ],
   "source": [
    "distinct_num_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    distinct_count = df.select(col).distinct().count()\n",
    "    distinct_num_counts[col] = distinct_count\n",
    "\n",
    "    print(f\"- {col} - {distinct_count} distinct values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Brew_Date - 9614455 distinct values\n",
      "- Beer_Style - 8 distinct values\n",
      "- SKU - 4 distinct values\n",
      "- Location - 10 distinct values\n"
     ]
    }
   ],
   "source": [
    "distinct_cat_counts = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    distinct_count = df.select(col).distinct().count()\n",
    "    distinct_cat_counts[col] = distinct_count\n",
    "\n",
    "    print(f\"- {col} - {distinct_count} distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "columns = df.columns\n",
    "filtered_columns = [col for col in columns if df.select(lower(col)).distinct().count() > 10]\n",
    "\n",
    "filtered_columns.remove('Brew_Date')\n",
    "filtered_columns.remove('Batch_ID')\n",
    "print(filtered_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewennes - Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "for col in filtered_columns:\n",
    "    df.select(skewness(col), kurtosis(col)).show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kurt_col = [\"USD_per_Liter\",\"Brewing_efficency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's analyze the skewness and kurtosis values you've provided:\n",
    "\n",
    "Understanding Skewness\n",
    "\n",
    "    Positive Skewness: Indicates a tail stretching towards the right of the distribution (more observations with higher values).\n",
    "    Negative Skewness: Indicates a tail stretching towards the left of the distribution (more observations with lower values).\n",
    "    Zero (or Near-Zero) Skewness: Suggests a relatively symmetrical distribution.\n",
    "\n",
    "Understanding Kurtosis\n",
    "\n",
    "    Normal Distribution (Mesokurtic): Has a kurtosis around 0.\n",
    "    High Kurtosis (Leptokurtic): Indicates a sharper peak and heavier tails compared to a normal distribution (more outliers).\n",
    "    Low Kurtosis (Platykurtic): Indicates a flatter peak and lighter tails compared to a normal distribution (fewer outliers).\n",
    "\n",
    "Analyzing Your Data\n",
    "\n",
    "Here's a breakdown of what the skewness and kurtosis values suggest about your features:\n",
    "\n",
    "Generally:\n",
    "\n",
    "    Skewness: Most of your features have slightly positive skewness, meaning the data distribution might lean slightly towards higher values. However, the skewness values are close to zero for many features, suggesting relatively balanced distributions.\n",
    "    Kurtosis: All your features exhibit negative kurtosis, indicating that they might have flatter distributions than a standard normal distribution with fewer outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skeweness visualization - when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# pdf = df.select('color').toPandas()\n",
    "\n",
    "# # Create the count plot using seaborn\n",
    "# import seaborn as sns\n",
    "# sns.countplot(x='color', data=pdf, order=pdf['color'].value_counts().index) \n",
    "# plt.title(\"Distribution of Color Feature\")\n",
    "# plt.xticks(rotation=45)  # Rotate x labels if they are long\n",
    "# plt.show()\n",
    "\n",
    "# # Density Plot\n",
    "# sns.kdeplot(pdf['color'])\n",
    "# plt.title(\"Density Plot of Color Feature (Negative Skew)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile-Quantile plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #more sparky approach\n",
    "\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def q_q_plot_pyspark(df, filtered_columns):\n",
    "#     for col in filtered_columns:\n",
    "#         quantiles = df.approxQuantile(col, [0.0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.0], 0.0)  \n",
    "\n",
    "#         # Sample data for plotting on the theoretical normal distribution\n",
    "#         theoretical_quantiles = F.array([F.lit(i) for i in quantiles]).cast(DoubleType())\n",
    "#         std_norm_quantiles = F.expr(\"percentile_approx(array(0.0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.0), 0.5)\")\n",
    "\n",
    "#         # QQ plot comparison\n",
    "#         qqplot_data = df.select(col, theoretical_quantiles, std_norm_quantiles)\n",
    "\n",
    "#         # Convert Spark DataFrame to Pandas, this will likely be more manageable\n",
    "#         pandas_df = qqplot_data.toPandas()\n",
    "\n",
    "#         # Plotting\n",
    "#         plt.figure()\n",
    "#         plt.scatter(pandas_df[col], pandas_df['std_norm_quantiles'])\n",
    "#         plt.xlabel(f\"Sample Quantiles ({col})\")\n",
    "#         plt.ylabel(\"Theoretical Normal Quantiles\")\n",
    "#         plt.title(f\"QQ Plot for Column: {col}\")\n",
    "#         plt.show()\n",
    "\n",
    "# q_q_plot_pyspark(df, filtered_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import statsmodels.api as sm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def q_q_plot(df, filtered_columns):\n",
    "#     for col in filtered_columns:\n",
    "#         fig, ax = plt.subplots()\n",
    "#         sm.qqplot(df.select(col).toPandas()[col], line='s', ax=ax)\n",
    "\n",
    "#         # Add title with column name\n",
    "#         ax.set_title(f\"QQ Plot for Column: {col}\") \n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "# q_q_plot(df, filtered_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Q-Q Plot:\n",
    "\n",
    "    Normal Distribution: If the data points closely follow the diagonal line, the distribution is likely close to normal.\n",
    "    Negative Skew (Long Left Tail): If the points curve below the line on the left side, it indicates a negative skew.\n",
    "    Positive Skew (Long Right Tail): If the points curve above the line on the right side, it indicates a positive skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not needed misinterpreted results from skewenss due to truncated show df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F.log1p() function is a PySpark function that calculates the natural logarithm of a column value plus one. It is commonly used to transform skewed or heavily right-skewed data to a more normalized distribution. By adding one to each value before taking the logarithm, it ensures that the function can handle zero or negative values without throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_skew_col = [\"pH_Level\", \"Gravity\", \"Alcohol_Content\"]\n",
    "\n",
    "# transformed_df = df.alias(\"transformed_df\")\n",
    "\n",
    "# transformed_df = transformed_df.withColumn(\"pH_Level\", F.log1p(\"pH_Level\"))\n",
    "# transformed_df = transformed_df.withColumn(\"Gravity\", F.log1p(\"Gravity\"))\n",
    "# transformed_df = transformed_df.withColumn(\"Alcohol_Content\", F.log1p(\"Alcohol_Content\"))\n",
    "\n",
    "# df.select(high_skew_col).show(5)\n",
    "# transformed_df.select(high_skew_col).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Ingredient_Ratio', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.0\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 0\n",
    "- Number of Outliers based on Gravity: 0\n",
    "- Number of Outliers based on Alcohol_Content: 0\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n",
    "- Number of Outliers based on grains: 0\n",
    "- Number of Outliers based on hops: 0\n",
    "- Number of Outliers based on USD_per_Liter: 250211\n",
    "- Number of Outliers based on Brewing_efficency: 247848\n",
    "\n",
    "##### Threshold 2.0 / 2.5 - (same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, abs, sqrt, avg, pow, when\n",
    "\n",
    "def detect_outliers_zscore(df, feature_col, threshold=3.0):\n",
    "    mean_val = df.select(avg(df[feature_col])).collect()[0][0] \n",
    "    std_dev = df.select(sqrt(avg(pow(df[feature_col] - mean_val, 2)))).collect()[0][0]  \n",
    "\n",
    "    return df.withColumn(\"outlier_\" + feature_col, \n",
    "                          when(abs((df[feature_col] - mean_val) / std_dev) > threshold, 1).otherwise(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in filtered_columns:\n",
    "    outlier = detect_outliers_zscore(df, col)\n",
    "    outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "    # Filtering directly on the 'outlier' DataFrame \n",
    "    outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "    print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier = detect_outliers_zscore(df, \"Bitterness\")\n",
    "# outlier_temp = outlier.filter(col(\"outlier_Bitterness\") == 1).count()\n",
    "# print(f\"Number of Outliers based on Bitterness: {outlier_temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  modified z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Ingredient_Ratio', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.5\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 0\n",
    "- Number of Outliers based on Gravity: 0\n",
    "- Number of Outliers based on Alcohol_Content: 0\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n",
    "- Number of Outliers based on grains: 0\n",
    "- Number of Outliers based on hops: 0\n",
    "- Number of Outliers based on USD_per_Liter: 428330\n",
    "- Number of Outliers based on Brewing_efficency: 1397253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, abs, median, sqrt, avg, pow\n",
    "\n",
    "# def detect_outliers_modified_zscore(df, feature_col, threshold=3.5):\n",
    "#     median_val = df.select(df[feature_col]).collect()[0][0] \n",
    "#     mad = df.select(median(abs(df[feature_col] - median_val))).collect()[0][0]\n",
    "\n",
    "#     return df.withColumn(\"outlier_\" + feature_col, \n",
    "#                           when(abs(0.6745 * (df[feature_col] - median_val) / mad) > threshold, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in filtered_columns:\n",
    "#     outlier = detect_outliers_modified_zscore(df, col)\n",
    "#     outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "#     # Filtering directly on the 'outlier' DataFrame \n",
    "#     outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "#     print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  modified z-score - adaptive threshold - i doubt the statistical right of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.5\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 3556568\n",
    "- Number of Outliers based on Gravity: 9535830\n",
    "- Number of Outliers based on Alcohol_Content: 189356\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_modified_zscore_adaptive(df, feature_col, threshold_factor=2.0):\n",
    "\n",
    "    median_val = df.select(df[feature_col]).collect()[0][0] \n",
    "    mad = df.select(median(abs(df[feature_col] - median_val))).collect()[0][0]\n",
    "\n",
    "    std_dev = np.std(df.select(feature_col).toPandas()[feature_col])  # Assuming numerical column \n",
    "    adaptive_threshold =  threshold_factor * std_dev\n",
    "\n",
    "    return df.withColumn(\"outlier_\" + feature_col, \n",
    "                          when(abs(0.6745 * (df[feature_col] - median_val) / mad) > adaptive_threshold, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns.remove(\"Ingredient_Ratio\")\n",
    "\n",
    "for col in filtered_columns:\n",
    "    outlier = detect_outliers_modified_zscore_adaptive(df, col)\n",
    "    outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "    # Filtering directly on the 'outlier' DataFrame \n",
    "    outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "    print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-values continous variables analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, dense_rank, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def ordinal_encode_pyspark(df, input_col, output_col):\n",
    "    \"\"\" Ordinal encodes a column within a PySpark DataFrame\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input Spark DataFrame.\n",
    "        input_col (str): Name of the column to be encoded. Defaults to \"Bitterness\".\n",
    "        output_col (str): Name for the new ordinal encoded column. Defaults to \"Bitterness_ordinal\".\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The Spark DataFrame with the ordinal encoded column.\n",
    "    \"\"\"\n",
    "\n",
    "    window = Window.orderBy(input_col) \n",
    "    return df.withColumn(output_col, dense_rank().over(window)) \\\n",
    "             .withColumn(output_col, when(col(input_col).isNull(), 0).otherwise(col(output_col))) \\\n",
    "             #.drop(input_col)\n",
    "             \n",
    "def check_distinct_values(df, feature_col):\n",
    "    \"\"\"  \n",
    "    Prints the distinct values of a feature in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The PySpark DataFrame.\n",
    "        feature_col (str): The name of the feature column.\n",
    "    \"\"\"\n",
    "\n",
    "    distinct_values = df.select(feature_col).distinct().collect()\n",
    "\n",
    "    print(f\"Distinct values in '{feature_col}':\")\n",
    "    for row in distinct_values:\n",
    "        print(row[feature_col]) \n",
    "\n",
    "    # functions\n",
    "\n",
    "def one_hot_encode_column(df, input_col, output_col_prefix=\"encoded\"):\n",
    "    \"\"\" One-hot encodes a specified categorical column, drops intermediary columns,\n",
    "        and allows for specifying the prefix of the output columns.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input DataFrame.\n",
    "        input_col (str): The name of the column containing categorical data.\n",
    "        output_col_prefix (str): Prefix for the generated one-hot encoded column names.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame with the one-hot encoded column(s) and \n",
    "                               intermediary columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    indexer = StringIndexer(inputCol=input_col, outputCol=input_col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCol=input_col + \"_index\", outputCol=output_col_prefix + input_col + \"features\")\n",
    "    assembler = VectorAssembler(inputCols=[input_col + \"_index\"], outputCol=\"features\") \n",
    "\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, encoder])\n",
    "    transformed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [input_col + \"_index\", \"features\"]\n",
    "    return transformed_df.drop(*drop_cols)  # Dynamic column dropping\n",
    "\n",
    "def discretize_and_encode(df, input_col=\"fermentation_time\"):\n",
    "    \"\"\" Discretizes a feature into bins, one-hot encodes the bins, and preserves the original column\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input DataFrame\n",
    "        input_col (str): Name of the column to be discretized and encoded\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame:  DataFrame with new discretization and encoded columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(input_col + \"_bin\", \n",
    "                       when((col(input_col) >= 10) & (col(input_col) <= 13), \"Short\")\n",
    "                       .when((col(input_col) >= 14) & (col(input_col) <= 16), \"Medium\")\n",
    "                       .otherwise(\"Long\"))\n",
    "    \n",
    "    return one_hot_encode_column(df, input_col=input_col + \"_bin\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sample = df.select(\"Color\").sample(fraction=0.1, seed=42).alias(\"color_sample\")\n",
    "color_frequency = color_sample.groupBy(\"Color\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=color_frequency, x=\"Color\", y=\"count\", order=color_frequency.sort_values('count', ascending=False)[\"Color\"])\n",
    "plt.title('Frequency of Color in the Dataset')\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ordinal_encode_pyspark(sample, \"Color\", \"Color_ordinal\")\n",
    "# sample.show(5)\n",
    "\n",
    "# summary_stats = sample.select(\"Color_ordinal\").toPandas().describe()\n",
    "# rounded_describe = np.round(summary_stats, 2)\n",
    "# rounded_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_frequency = sample.groupBy(\"Color_ordinal\").count().toPandas()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(data=color_frequency, x=\"Color_ordinal\", y=\"count\", order=color_frequency.sort_values('Color_ordinal', ascending=True)[\"Color_ordinal\"])\n",
    "# plt.title('Frequency of Color in the Dataset')\n",
    "# plt.xlabel('Color')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bitterness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitterness_sample = df.select(\"Bitterness\").sample(fraction=0.1, seed=42).alias(\"bitterness_sample\")\n",
    "Bitterness_frequency = bitterness_sample.groupBy(\"Bitterness\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=Bitterness_frequency, x=\"Bitterness\", y=\"count\", order=Bitterness_frequency.sort_values('Bitterness', ascending=True)[\"Bitterness\"])\n",
    "plt.title('Frequency of Bitterness in the Dataset')\n",
    "plt.xlabel('Bitterness')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ordinal_encode_pyspark(sample, \"Bitterness\", \"Bitterness_ordinal\") \n",
    "# sample.show(5)\n",
    "\n",
    "# summary_stats = sample.select(\"Bitterness_ordinal\").toPandas().describe()\n",
    "# rounded_describe = np.round(summary_stats, 2)\n",
    "# rounded_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitterness_ordinal_frequency = sample.groupBy(\"Bitterness_ordinal\").count().toPandas()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(data=Bitterness_ordinal_frequency, x=\"Bitterness_ordinal\", y=\"count\", order=Bitterness_ordinal_frequency.sort_values('Bitterness_ordinal', ascending=True)[\"Bitterness_ordinal\"])\n",
    "# plt.title('Frequency of Bitterness_ordinal in the Dataset')\n",
    "# plt.xlabel('Bitterness_ordinal')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_sample = df.select(\"grains\").sample(fraction=0.1, seed=42).alias(\"grain_sample\")\n",
    "grains_frequency = grain_sample.groupBy(\"grains\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=grains_frequency, x=\"grains\", y=\"count\", order=grains_frequency.sort_values('grains', ascending=True)[\"grains\"])\n",
    "plt.title('Frequency of Styles')\n",
    "plt.xlabel('Beer_Style')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hops_sample = df.select(\"hops\").sample(fraction=0.1, seed=42).alias(\"hops_sample\")\n",
    "hops_frequency = hops_sample.groupBy(\"hops\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=hops_frequency, x=\"hops\", y=\"count\", order=hops_frequency.sort_values('hops', ascending=True)[\"hops\"])\n",
    "plt.title('Frequency of Hops')\n",
    "plt.xlabel('Hops')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fermentation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretize and encode is hardcoded for fermentation time (!!)\n",
    "\n",
    "df = discretize_and_encode(df)\n",
    "df.select(\"encodedfermentation_time_binfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedfermentation_time_binfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beer style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column encodedBeer_Stylefeatures already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBeer_Style\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencodedBeer_Stylefeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 62\u001b[0m, in \u001b[0;36mone_hot_encode_column\u001b[1;34m(df, input_col, output_col_prefix)\u001b[0m\n\u001b[0;32m     59\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[input_col \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     61\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[indexer, assembler, encoder])\n\u001b[1;32m---> 62\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Drop unnecessary columns\u001b[39;00m\n\u001b[0;32m     65\u001b[0m drop_cols \u001b[38;5;241m=\u001b[39m [input_col \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:/Spark/spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column encodedBeer_Stylefeatures already exists."
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"Beer_Style\")\n",
    "df.select(\"encodedBeer_Stylefeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedBeer_Stylefeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|encodedSKUfeatures|\n",
      "+------------------+\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|     (3,[2],[1.0])|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"SKU\")\n",
    "df.select(\"encodedSKUfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedSKUfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|encodedLocationfeatures|\n",
      "+-----------------------+\n",
      "|              (9,[],[])|\n",
      "|              (9,[],[])|\n",
      "|          (9,[7],[1.0])|\n",
      "|          (9,[4],[1.0])|\n",
      "|          (9,[3],[1.0])|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"Location\")\n",
    "df.select(\"encodedLocationfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedLocationfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1987"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month , to_date,  col\n",
    "\n",
    "plot_sample = df.alias(\"plot_sample\")\n",
    "\n",
    "plot_sample = plot_sample.withColumn(\"Year\", year(\"Brew_Date\")).withColumn(\"Month\", month(\"Brew_Date\"))\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "print(f'We have data in a range from {plot_sample.select(F.min(\"year\")).collect()[0][0]} to {plot_sample.select(F.max(\"year\")).collect()[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=plot_sample.select(\"USD_per_Liter\").toPandas(), x=\"USD_per_Liter\", kde=True)\n",
    "\n",
    "plt.title(f'Histogram for USD_per_Liter')\n",
    "plt.xlabel(\"USD_per_Liter\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "key_mode_count = plot_sample.groupBy('Color', 'Beer_Style').count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size to 10 inches wide and 5 inches tall\n",
    "sns.barplot(x=\"Color\", y=\"count\", data=key_mode_count, hue='Beer_Style')\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "#that is conceptually wrong there cant be the same distribution of type of beers for each color basically impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('SKU', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for SKU in pandas_df['SKU'].unique():\n",
    "   data = pandas_df[pandas_df['SKU'] == SKU]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=SKU)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by SKU\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('Beer_Style', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for Beer_Style in pandas_df['Beer_Style'].unique():\n",
    "   data = pandas_df[pandas_df['Beer_Style'] == Beer_Style]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=Beer_Style)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by Beer_Style\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date,mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('Location', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for location in pandas_df['Location'].unique():\n",
    "   data = pandas_df[pandas_df['Location'] == location]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=location)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by Location\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Calculation - Aggregate total sales by date \n",
    "overall_sales_trend = plot_sample.groupBy('Brew_Date') \\\n",
    "                        .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                        .orderBy('Brew_Date')\n",
    "\n",
    "# Convert to pandas and plot\n",
    "pandas_df = overall_sales_trend.toPandas()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pandas_df['Brew_Date'], pandas_df['Total_Sales'])\n",
    "\n",
    "# Customize plot elements \n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Overall Sales Trend\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Calculation - Aggregate total sales by date \n",
    "overall_sales_trend = plot_sample.groupBy('Brew_Date') \\\n",
    "                        .agg(mean('Temperature').alias('Temperature')) \\\n",
    "                        .orderBy('Brew_Date')\n",
    "\n",
    "# Convert to pandas and plot\n",
    "pandas_df = overall_sales_trend.toPandas()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pandas_df['Brew_Date'], pandas_df['Temperature'])\n",
    "\n",
    "# Customize plot elements \n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.title(\"Overall Temperature Trend\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date,mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('SKU', 'Brew_Date') \\\n",
    "                           .agg(mean('Fermentation_Time').alias('Fermentation_Time')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for SKU in pandas_df['SKU'].unique():\n",
    "   data = pandas_df[pandas_df['SKU'] == SKU]\n",
    "   plt.plot(data['Brew_Date'], data['Fermentation_Time'], label=SKU)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Fermentation_Time \")\n",
    "plt.title(\"Fermentation_Time Trends by SKU\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample = plot_sample.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### losses against other\n",
    "\n",
    "- brewhouse efficency (no patterns \\ uniform)\n",
    "- usd per liter (no patterns \\ uniform)\n",
    "- year (no patterns \\ uniform)\n",
    "- month (no patterns \\ uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,4))  # Set the figure size to 10 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Brewing\", y=\"Brewhouse_Efficiency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))  # Set the figure size to 10 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Fermentation\", y=\"Year\")\n",
    "plt.yticks([2020,2021,2022,2023])\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size to 12 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Bottling_Kegging\", y=\"Month\")\n",
    "plt.yticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quality scores against other\n",
    "\n",
    "- Color (no patterns \\ uniform)\n",
    "- Alcohol_Content (no patterns \\ uniform)\n",
    "- Bitterness (no patterns \\ uniform)\n",
    "- volume produced (no patterns \\ uniform)\n",
    "- total sales (no patterns \\ uniform)\n",
    "- brewhouse efficency (no patterns \\ uniform)\n",
    "- usd per liter (no patterns \\ uniform)\n",
    "\n",
    "\n",
    "add to the quality score plotting all the features that are strictly related to the fermentation process also aginst losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"USD_per_Liter\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Fermentation_Time\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Temperature\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"pH_Level\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Gravity\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Alcohol_Content\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Bitterness\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Volume_Produced\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Brewing\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Fermentation\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Bottling_Kegging\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"grains\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"hops\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample = df.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample = corr_sample.drop(\"water\", \"Batch_ID\",\"encodedBeer_Stylefeatures\", \"encodedSKUfeatures\", \"encodedLocationfeatures\")\n",
    "corr_sample = corr_sample.drop(\"Brew_Date\", \"encodedfermentation_time_binfeatures\")\n",
    "corr_sample = corr_sample.drop(\"Beer_Style\", \"SKU\", \"Location\", \"fermentation_time_bin\")\n",
    "corr_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.corr(method='pearson')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.corr(method='spearman')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_sample.toPandas().corr(method='kendall')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# features = corr_sample.schema.names\n",
    "# vectorassembler = VectorAssembler(inputCols = features, outputCol= 'assemblerfeatures')\n",
    "# output_dataset = vectorassembler.transform(corr_sample)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.stat import Correlation\n",
    "\n",
    "# pearsonCorr = Correlation.corr(output_dataset, 'assemblerfeatures', 'pearson').collect()[0][0]\n",
    "\n",
    "# #trasformo la DenseMatrix in un array numpy\n",
    "# correlation_array = pearsonCorr.toArray() #ritorna un numpy.ndarray\n",
    "\n",
    "# correlationDF = pd.DataFrame(\n",
    "#     correlation_array,\n",
    "#     index = features,\n",
    "#     columns = features\n",
    "# )\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(15,12)})\n",
    "# sns.heatmap(correlationDF, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MachineLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss-Focused Prediction: Could you set up regression or classification models to predict if a batch tends to have higher brewing losses based on brew conditions (temperature, etc.)? This might provide insight on process adjustments to decrease loss percentage.\n",
    "Efficiency Gains: What combinations of brewing factors have produced the highest Brewhouse Efficiency outcomes? Analyzing this could assist in streamlining processes for optimizing ingredient use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alcohol content\n",
    "\n",
    "    - features = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "                 \"Bitterness\", \"Color\", \"grains\", \"hops\"]\n",
    "    - Regression on alcohol content\n",
    "    - Classficiation on alcohol content bin\n",
    "    - Anomaly detection on alcohol content\n",
    "\n",
    "- Loss during processes\n",
    "\n",
    "- Quality score & usdperliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1001591\n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(fraction=0.1, seed=42) #.drop(\"Batch_ID\")\n",
    "print(f\"Sample size: {sample.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batch_ID: long (nullable = true)\n",
      " |-- Brew_Date: string (nullable = true)\n",
      " |-- Beer_Style: string (nullable = true)\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Fermentation_Time: long (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- pH_Level: double (nullable = true)\n",
      " |-- Gravity: double (nullable = true)\n",
      " |-- Alcohol_Content: double (nullable = true)\n",
      " |-- Bitterness: long (nullable = true)\n",
      " |-- Color: long (nullable = true)\n",
      " |-- Volume_Produced: long (nullable = true)\n",
      " |-- Total_Sales: double (nullable = true)\n",
      " |-- Quality_Score: double (nullable = true)\n",
      " |-- Brewhouse_Efficiency: double (nullable = true)\n",
      " |-- Loss_During_Brewing: double (nullable = true)\n",
      " |-- Loss_During_Fermentation: double (nullable = true)\n",
      " |-- Loss_During_Bottling_Kegging: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- grains: double (nullable = true)\n",
      " |-- hops: double (nullable = true)\n",
      " |-- USD_per_Liter: double (nullable = true)\n",
      " |-- Brewing_efficency: double (nullable = true)\n",
      " |-- encodedBeer_Stylefeatures: vector (nullable = true)\n",
      " |-- encodedSKUfeatures: vector (nullable = true)\n",
      " |-- encodedLocationfeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+-------+------------+-----------------+------------------+-----------------+------------------+-----------------+----------+-----+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+-----+------+----+-------------------+--------------------+-------------------------+------------------+-----------------------+\n",
      "|Batch_ID|          Brew_Date|Beer_Style|    SKU|    Location|Fermentation_Time|       Temperature|         pH_Level|           Gravity|  Alcohol_Content|Bitterness|Color|Volume_Produced|       Total_Sales|    Quality_Score|Brewhouse_Efficiency|Loss_During_Brewing|Loss_During_Fermentation|Loss_During_Bottling_Kegging|water|grains|hops|      USD_per_Liter|   Brewing_efficency|encodedBeer_Stylefeatures|encodedSKUfeatures|encodedLocationfeatures|\n",
      "+--------+-------------------+----------+-------+------------+-----------------+------------------+-----------------+------------------+-----------------+----------+-----+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+-----+------+----+-------------------+--------------------+-------------------------+------------------+-----------------------+\n",
      "| 8843420|2020-01-01 00:01:51|Wheat Beer|   Cans| Indiranagar|               13| 21.60546968919005|4.507213419450749|1.0415809900303394| 4.83702482975061|        38|    9|           4949|1082.3549117830858|6.295632960105362|   73.43876487534912| 1.7047005571803084|      1.7187729863809476|          1.0336558740315214|  1.0|  0.45|0.26|0.21870174010569524|0.014839111916619342|                (7,[],[])|     (3,[2],[1.0])|          (9,[5],[1.0])|\n",
      "| 1816588|2020-01-01 00:03:22|      Sour|Bottles|Marathahalli|               11|22.903972245938505|5.473563790227288|1.0373730025224546| 5.12916808476057|        44|   10|           3377| 13527.91912082282|9.167866793350631|   75.82965914771283|   2.28403754459136|       1.442312900085096|          2.1676876404222445|  1.0|  0.31|0.21|  4.005898466337821|0.022454740641904895|            (7,[2],[1.0])|     (3,[0],[1.0])|          (9,[3],[1.0])|\n",
      "| 3420320|2020-01-01 00:03:31|   Pilsner|  Pints| Rajajinagar|               15|17.785713321899884|5.318024913782187|1.0687167841553329|5.406625556398892|        23|    5|           3809| 5925.879914754564|9.181668960937603|   88.37087230231108|  2.583703605893361|      3.1816797935563645|           2.301820283939322|  1.0|  0.23|0.16| 1.5557573942647844|0.023200544054164107|            (7,[6],[1.0])|     (3,[1],[1.0])|          (9,[4],[1.0])|\n",
      "| 9790483|2020-01-01 00:10:42|Wheat Beer|   Kegs|  HSR Layout|               16|24.869178082298525|4.849521382421813|1.0451318832513898|4.992812931715872|        40|   10|           2436|  4512.73451819818| 8.72165976065684|   88.65599038593423|  1.487622328596904|       4.103369597188115|           3.260870878389481|  1.0|  0.42|0.27| 1.8525182751224056|0.036394084723289914|                (7,[],[])|         (3,[],[])|          (9,[0],[1.0])|\n",
      "| 1709420|2020-01-01 00:12:06|    Porter|   Kegs|   Yelahanka|               12|22.629383129373903| 4.77639650569921|1.0305953857694026|5.934652916760902|        38|   19|            960| 15506.14744870443|9.544811705657294|   85.22587157203377| 1.1588221289766367|      2.0246751904794893|           4.977855837624096|  1.0|  0.26|0.11|  16.15223692573378| 0.08877694955420184|            (7,[1],[1.0])|         (3,[],[])|          (9,[6],[1.0])|\n",
      "+--------+-------------------+----------+-------+------------+-----------------+------------------+-----------------+------------------+-----------------+----------+-----+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+-----+------+----+-------------------+--------------------+-------------------------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alcohol content prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.001591e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.250250e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.329737e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.500002e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.875388e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.250748e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.625394e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.999999e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alcohol_content\n",
       "count     1.001591e+06\n",
       "mean      5.250250e+00\n",
       "std       4.329737e-01\n",
       "min       4.500002e+00\n",
       "25%       4.875388e+00\n",
       "50%       5.250748e+00\n",
       "75%       5.625394e+00\n",
       "max       5.999999e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.select(\"Alcohol_content\").toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Feature Preparation\n",
    "# features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "#                  \"Bitterness\", \"Color\", \"grains\", \"hops\"]\n",
    "\n",
    "# # Dataset Preparation\n",
    "# regr_alc_sample = df.alias(\"regr_alc_sample\")\n",
    "# regr_alc_train, regr_alc_test = regr_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "# train_df_transformed = assembler.transform(regr_alc_train)\n",
    "# test_df_transformed = assembler.transform(regr_alc_test)\n",
    "\n",
    "# # Standardization (Optional)\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "# scaler_model = scaler.fit(train_df_transformed)\n",
    "# train_df_transformed = scaler_model.transform(train_df_transformed)\n",
    "# test_df_transformed = scaler_model.transform(test_df_transformed)\n",
    "\n",
    "# # Linear Regression Model\n",
    "# lr = LinearRegression(featuresCol=\"features\", labelCol=\"Alcohol_Content\")\n",
    "# lr_model = lr.fit(train_df_transformed)\n",
    "\n",
    "# # Transformation\n",
    "# train_predictions = lr_model.transform(train_df_transformed)\n",
    "# test_predictions = lr_model.transform(test_df_transformed)\n",
    "\n",
    "# # Evaluation (RMSE)\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Alcohol_Content\", predictionCol=\"prediction\")\n",
    "# train_rmse = evaluator.evaluate(train_predictions)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "# print(f\"Train RMSE: {train_rmse}\")\n",
    "# print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "# # Model Coefficients (Feature Influence)\n",
    "# print(lr_model.coefficients) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Feature Preparation\n",
    "# features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "#                 \"Bitterness\", \"Color\", \"grains\", \"hops\"]\n",
    "\n",
    "# # 1. Vector Assembler:\n",
    "# assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "\n",
    "# # 2. Standard Scaler (If you decide to standardize):\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# # 3. Linear Regression Model:\n",
    "# lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"Alcohol_Content\") # FeaturesCol matches Scaler output\n",
    "\n",
    "\n",
    "# # Pipeline for Modeling Stages Only\n",
    "# pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# # Dataset Preparation\n",
    "# regr_alc_sample = sample.alias(\"regr_alc_sample\")\n",
    "# regr_alc_train, regr_alc_test = regr_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# # Fit the Pipeline:\n",
    "# pipeline_model = pipeline.fit(regr_alc_train)\n",
    "\n",
    "# # Transform (for predictions on both train and test)\n",
    "# train_predictions = pipeline_model.transform(regr_alc_train)\n",
    "# test_predictions = pipeline_model.transform(regr_alc_test)\n",
    "\n",
    "# # Evaluation Stage (after getting predictions)\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Alcohol_Content\", predictionCol=\"prediction\")\n",
    "# train_rmse = evaluator.evaluate(train_predictions)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# print(\"Train Root Mean Squared Error (RMSE): %f\" % train_rmse) \n",
    "# print(\"Test Root Mean Squared Error (RMSE): %f\" % test_rmse) \n",
    "\n",
    "# # Access Coefficients\n",
    "# lr_stage = pipeline_model.stages[2] # Access the fitted LinearRegression model\n",
    "# print(lr_stage.coefficients) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "# Feature Preparation\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\"]\n",
    "\n",
    "# 1. Vector Assembler:\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"Alcohol_Content\") # FeaturesCol matches Scaler output\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# Grid Search for Hyperparameter Tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"Alcohol_Content\", predictionCol=\"prediction\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Dataset Preparation\n",
    "regr_alc_sample = df.alias(\"regr_alc_sample\")\n",
    "regr_alc_train, regr_alc_test = regr_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the Pipelines:\n",
    "model = crossval.fit(regr_alc_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = model.transform(regr_alc_train)\n",
    "test_predictions = model.transform(regr_alc_test)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Root Mean Squared Error (RMSE): %f\" % train_rmse)\n",
    "print(\"Test Root Mean Squared Error (RMSE): %f\" % test_rmse)\n",
    "\n",
    "# Access Coefficients\n",
    "best_lr_model = model.stages[-1].bestModel\n",
    "print(best_lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting Results\n",
    "\n",
    "    RMSE: Evaluate the RMSE score. A lower value implies better average accuracy in predicting alcohol content using your features.\n",
    "    \n",
    "    Coefficients: Examine the sign (+/-) and magnitude of each feature's coefficient. These tell you:\n",
    "        Positive Coefficient: Increases in this feature tend to associate with higher 'Alcohol_Content'.\n",
    "        Negative Coefficient: Increases in this feature tend to associate with lower 'Alcohol_Content'.\n",
    "        Magnitude: Larger the value (ignoring the sign), stronger the apparent impact of said feature, linearly speaking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Results\n",
    "\n",
    "    RMSE (0.432971): The context is crucial here. To judge whether this RMSE is \"good\" or \"bad,\" consider the natural range of 'Alcohol_Content' values in your dataset.\n",
    "        As a crude example, suppose 'Alcohol_Content' typically varies between 4% and 10%. Then a 0.43 error might be tolerable.\n",
    "        If these alcohol values instead range from 4% to 5%, this same error becomes highly significant.\n",
    "\n",
    "Analyzing Coefficients\n",
    "\n",
    "Here's a breakdown of how to draw inferences from the model's coefficients:\n",
    "\n",
    "    Sign:\n",
    "        Positive coefficients suggest those features tend to be associated with higher alcohol content in your dataset.\n",
    "        Negative coefficients imply an association with lower alcohol content.\n",
    "\n",
    "    Magnitude: The larger the absolute value of a coefficient, the stronger its apparent linear influence on 'Alcohol_Content' is, all else being equal.\n",
    "\n",
    "    Sparseness: Notice several coefficients (especially those for ingredient amounts) are very close to zero. This might mean these have practically negligible linear impact within the range of values encountered in your dataset.\n",
    "\n",
    "Considerations\n",
    "\n",
    "    Statistical Significance: This simple linear regression doesn't provide measures of statistical significance for each coefficient. Techniques exist to estimate confidence intervals, making more robust assertions about feature effect strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Metrics\n",
    "Python\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_true = regr_brewloss_test['Loss_During_Brewing'] # Actual Values\n",
    "y_pred = test_predictions.select('prediction').collect() # Model Predictions\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False) \n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print('R-squared:', r2)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae) \n",
    "\n",
    "\n",
    "Visualizing the Tree\n",
    "Python\n",
    "\n",
    "from sklearn import tree \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8)) \n",
    "tree.plot_tree(your_dt_model, feature_names=features_cols, filled=True) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "Feature Importance\n",
    "Python\n",
    "\n",
    "feature_importances = your_dt_model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1] # To show in descending order\n",
    "\n",
    "print('Feature Importances:')\n",
    "for f in range(len(features_cols)):\n",
    "   print(features_cols[sorted_indices[f]], ':', feature_importances[sorted_indices[f]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "udf explanation site: https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "#                  \"Bitterness\", \"grains\", \"hops\",]\n",
    "\n",
    "# # Dataset preparation\n",
    "# tree_alc_sample = sample.alias(\"tree_alc_sample\")\n",
    "# tree_alc_train, tree_alc_test = tree_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# # Binarizing 'Alcohol_Content'\n",
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# def binarize(alcohol_value):\n",
    "#     if alcohol_value  <= 5.0:\n",
    "#         return \"Low\"\n",
    "#     elif alcohol_value <= 5.5:\n",
    "#         return \"Medium\"\n",
    "#     else:\n",
    "#         return \"High\"\n",
    "\n",
    "# binarize_udf = udf(binarize, StringType())\n",
    "# tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "# tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "\n",
    "# # Encoding 'Alcohol_Bin' this solves the stringtype issue for the onehotencoder\n",
    "# tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "#                         otherwise(2))\n",
    "\n",
    "# tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "#                         otherwise(2)) \n",
    "\n",
    "# # Feature Preparation\n",
    "# assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")  \n",
    "# train_df_transformed = assembler.transform(tree_alc_train)\n",
    "# test_df_transformed = assembler.transform(tree_alc_test)\n",
    "\n",
    "# # Decision Tree Classifier\n",
    "# dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Alcohol_Bin\")\n",
    "# dt_model = dt.fit(train_df_transformed)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = dt_model.transform(test_df_transformed)\n",
    "\n",
    "# # Evaluation (For Classification)\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"Alcohol_Bin\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy: %f\" % accuracy) \n",
    "\n",
    "# # Visualization (may need external libraries depending on your setup)\n",
    "# print(dt_model.toDebugString) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - pipeline crossval scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "\n",
    "\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "                 \"Bitterness\", \"grains\", \"hops\",]\n",
    "# Dataset preparation\n",
    "tree_alc_sample = df.alias(\"tree_alc_sample\")\n",
    "tree_alc_train, tree_alc_test = tree_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Binarizing 'Alcohol_Content'\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def binarize(alcohol_value):\n",
    "    if alcohol_value  <= 5.0:\n",
    "        return \"Low\"\n",
    "    elif alcohol_value <= 5.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "binarize_udf = udf(binarize, StringType())\n",
    "tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "\n",
    "# Encoding 'Alcohol_Bin' this solves the stringtype issue for the onehotencoder\n",
    "tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", \n",
    "                        when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "                        when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "                        otherwise(2))\n",
    "\n",
    "tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", \n",
    "                        when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "                        when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "                        otherwise(2)) \n",
    "\n",
    "# Feature Preparation\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")  \n",
    "\n",
    "# Standardization (Optional)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Alcohol_Bin\")\n",
    "\n",
    "# Parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 8]) \\\n",
    "    .addGrid(dt.maxBins, [16, 32]) \\\n",
    "    .addGrid(dt.impurity, ['gini', 'entropy']) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Alcohol_Bin\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Pipeline for Modeling Stages Only\n",
    "pipeline = Pipeline(stages=[assembler, crossval])\n",
    "\n",
    "# Fit the Pipelines:\n",
    "pipeline_model = pipeline.fit(tree_alc_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = pipeline_model.transform(tree_alc_train)\n",
    "test_predictions = pipeline_model.transform(tree_alc_test)\n",
    "\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Accuracy: %f\" % train_accuracy)\n",
    "print(\"Test Accuracy: %f\" % test_accuracy)\n",
    "\n",
    "# Access Best Model\n",
    "best_dt_model = pipeline_model.stages[1].bestModel\n",
    "print(best_dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - pipeline crossval scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "\n",
    "\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "                 \"Bitterness\", \"grains\", \"hops\",]\n",
    "\n",
    "# Dataset preparation\n",
    "tree_alc_sample = df.alias(\"tree_alc_sample\")\n",
    "tree_alc_train, tree_alc_test = tree_alc_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Binarizing 'Alcohol_Content'\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def binarize(alcohol_value):\n",
    "    if alcohol_value  <= 5.0:\n",
    "        return \"Low\"\n",
    "    elif alcohol_value <= 5.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "binarize_udf = udf(binarize, StringType())\n",
    "tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "\n",
    "# Encoding 'Alcohol_Bin' this solves the stringtype issue for the onehotencoder\n",
    "tree_alc_train = tree_alc_train.withColumn(\"Alcohol_Bin\", \n",
    "                        when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "                        when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "                        otherwise(2))\n",
    "\n",
    "tree_alc_test = tree_alc_test.withColumn(\"Alcohol_Bin\", \n",
    "                        when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "                        when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "                        otherwise(2)) \n",
    "\n",
    "# Feature Preparation\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")  \n",
    "\n",
    "# Standardization (Optional)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Alcohol_Bin\")\n",
    "\n",
    "# Parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 8]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32]) \\\n",
    "    .addGrid(rf.impurity, ['gini', 'entropy']) \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.subsamplingRate, [0.6, 0.8]) \\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Alcohol_Bin\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Pipeline for Modeling Stages Only\n",
    "pipeline = Pipeline(stages=[assembler, scaler, crossval])\n",
    "\n",
    "# Fit the Pipelines:\n",
    "pipeline_model = pipeline.fit(tree_alc_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = pipeline_model.transform(tree_alc_train)\n",
    "test_predictions = pipeline_model.transform(tree_alc_test)\n",
    "\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Accuracy: %f\" % train_accuracy)\n",
    "print(\"Test Accuracy: %f\" % test_accuracy)\n",
    "\n",
    "# Access Best Model\n",
    "best_dt_model = pipeline_model.stages[2].bestModel\n",
    "print(best_dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection - NOT DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization: To Scale or Not to Scale?\n",
    "\n",
    "    For Isolation Forests: Tree-based algorithms are generally less sensitive to feature scaling than distance-based ones like K-means. There might be a marginal benefit but it's not strictly mandatory.\n",
    "    Potential Benefit: Some of your features have relatively tight standard deviations ('Alcohol_Content', 'Gravity'), while others like 'Bitterness' and 'Volume_Produced' have more extensive spreads. Standardization could prevent features with larger scales from overpowering the model's analysis, especially if correlations exist.\n",
    "    Recommendation: I suggest experimenting with both standardized and non-standardized data with your Isolation Forest model. Compare the identified anomalies; if there's significant discrepancy, standardization likely improved interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False) \n",
    "# scalerModel = scaler.fit(df_transformed)\n",
    "# scaled_data = scalerModel.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import DecisionTreeClassifier, IsolationForest\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \n",
    "#                  \"Bitterness\", \"Color\", \"grains\", \"hops\", \"encodedBeer_Stylefeatures\"]\n",
    "\n",
    "# tree_alc_sample = sample.alias(\"tree_alc_sample\")\n",
    "\n",
    "# # Binarizing 'Alcohol_Content'\n",
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# def binarize(alcohol_value):\n",
    "#     if alcohol_value  <= 5.0:\n",
    "#         return \"Low\"\n",
    "#     elif alcohol_value <= 5.5:\n",
    "#         return \"Medium\"\n",
    "#     else:\n",
    "#         return \"High\"\n",
    "\n",
    "# binarize_udf = udf(binarize, StringType())\n",
    "# tree_alc_sample = tree_alc_sample.withColumn(\"Alcohol_Bin\", binarize_udf(\"Alcohol_Content\"))\n",
    "\n",
    "# # Encoding 'Alcohol_Bin' this solves the stringtype issue for the onehotencoder\n",
    "# tree_alc_sample = tree_alc_sample.withColumn(\"Alcohol_Bin\", \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Low\", 0). \n",
    "#                         when(col(\"Alcohol_Bin\") == \"Medium\", 1).\n",
    "#                         otherwise(2)) \n",
    "\n",
    "# # Feature Preparation\n",
    "# assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")  \n",
    "# df_transformed = assembler.transform(tree_alc_sample)\n",
    "\n",
    "# # Decision Tree Classifier\n",
    "# iforest = IsolationForest(featuresCol=\"features\", labelCol=\"Alcohol_Bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss during brewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first features batch = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \"Volume_Produced\", \"Brewhouse_Efficiency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr (r2) = \n",
    "- Train Root Mean Squared Error (r2): 0.000000 \n",
    "- Test Root Mean Squared Error (r2): -0.000015\n",
    "- [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "\n",
    "lr (mae) =\n",
    "- Train Root Mean Squared Error (mae): 0.999923\n",
    "- Test Root Mean Squared Error (mae): 0.998363\n",
    "- [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "\n",
    "dt (r2) = \n",
    "- Train Root Mean Squared Error (r2): 0.000203\n",
    "- Test Root Mean Squared Error (r2): -0.000283\n",
    "\n",
    "dt (mae) =\n",
    "- Train Root Mean Squared Error (mae): 0.999888\n",
    "- Test Root Mean Squared Error (mae): 0.998390\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second feature bacth = [\"Volume_Produced\",\"Total_Sales\", \"Quality_Score\", \"Brewhouse_Efficiency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr (r2) =\n",
    "- Train Root Mean Squared Error (r2): 0.000005\n",
    "- Test Root Mean Squared Error (r2): -0.000021\n",
    "- [3.8205088347048814e-07,2.258462545652026e-07,-0.0005469537426189472,-9.910365816318696e-05]\n",
    "\n",
    "lr (mae) = \n",
    "- Train Root Mean Squared Error (mae): 0.999918\n",
    "- Test Root Mean Squared Error (mae): 0.998364\n",
    "- [6.560648378543714e-07,3.879291685535881e-07,-0.0009406175959918453,-0.00017003566917370277]\n",
    "\n",
    "dt (r2) = \n",
    "- Train Root Mean Squared Error (r2): 0.000040\n",
    "- Test Root Mean Squared Error (r2): -0.000072\n",
    "\n",
    "dt (mae) =\n",
    "- Train Root Mean Squared Error (mae): 0.999895\n",
    "- Test Root Mean Squared Error (mae): 0.998379\n",
    "\n",
    "gbt (r2) = \n",
    "- Train Root Mean Squared Error (r2): 0.000126\n",
    "- Test Root Mean Squared Error (r2): -0.000090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, RobustScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Feature Preparation\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \"Brewhouse_Efficiency\"]\n",
    "\n",
    "# Dataset Preparation\n",
    "regr_brewloss_sample = df.alias(\"regr_brewloss_sample\")\n",
    "\n",
    "# Train-Test Split\n",
    "regr_brewloss_train, regr_brewloss_test = regr_brewloss_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 1. Vector Assembler:\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "\n",
    "# 2. Standard Scaler (If you decide to standardize):\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "# scaler = RobustScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# 3. Regression Models:\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "\n",
    "# Grid Search for Hyperparameter Tuning\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Loss_During_Brewing\", predictionCol=\"prediction\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Pipeline for Modeling Stages Only\n",
    "pipeline = Pipeline(stages=[assembler,scaler, crossval])\n",
    "\n",
    "# Fit the Pipelines:\n",
    "pipeline_model = pipeline.fit(regr_brewloss_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = pipeline_model.transform(regr_brewloss_train)\n",
    "test_predictions = pipeline_model.transform(regr_brewloss_test)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Root Mean Squared Error (mae): %f\" % train_rmse)\n",
    "print(\"Test Root Mean Squared Error (mae): %f\" % test_rmse)\n",
    "\n",
    "# Access Best Model\n",
    "best_lr_model = pipeline_model.stages[2].bestModel\n",
    "print(best_lr_model.coefficients)\n",
    "\n",
    "print(pipeline_model.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss during fermentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first features batch = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \"Volume_Produced\", \"Brewhouse_Efficiency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr (r2) = \n",
    "- Train Root Mean Squared Error (r2): 0.000000\n",
    "- Test Root Mean Squared Error (r2): -0.000002\n",
    "\n",
    "lr (mae) = \n",
    "- Train Root Mean Squared Error (mae): 0.999648\n",
    "- Test Root Mean Squared Error (mae): 1.002373\n",
    "\n",
    "\n",
    "dt (r2) = \n",
    "- Train Root Mean Squared Error (r2): -0.000026\n",
    "- Test Root Mean Squared Error (r2): -0.000016\n",
    "\n",
    "dt (mae) =\n",
    "- Train Root Mean Squared Error (mae): 0.999651\n",
    "- Test Root Mean Squared Error (mae): 1.002360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second feature bacth = [\"Volume_Produced\",\"Total_Sales\", \"Quality_Score\", \"Brewhouse_Efficiency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Feature Preparation\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \"Volume_Produced\", \"Brewhouse_Efficiency\"]\n",
    "\n",
    "# Dataset Preparation\n",
    "regr_fermloss_sample = df.alias(\"regr_fermloss_sample\")\n",
    "\n",
    "# Train-Test Split\n",
    "regr_fermloss_train, regr_fermloss_test = regr_fermloss_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 1. Vector Assembler:\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "\n",
    "# 2. Standard Scaler (If you decide to standardize):\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# 3. Regression Models:\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Loss_During_Fermentation\")\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "\n",
    "# Grid Search for Hyperparameter Tuning\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Loss_During_Fermentation\", predictionCol=\"prediction\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Pipeline for Modeling Stages Only\n",
    "pipeline = Pipeline(stages=[assembler,scaler, crossval])\n",
    "\n",
    "# Fit the Pipelines:\n",
    "pipeline_model = pipeline.fit(regr_brewloss_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = pipeline_model.transform(regr_fermloss_train)\n",
    "test_predictions = pipeline_model.transform(regr_fermloss_test)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Root Mean Squared Error (mae): %f\" % train_rmse)\n",
    "print(\"Test Root Mean Squared Error (mae): %f\" % test_rmse)\n",
    "\n",
    "# # Access Best Model\n",
    "# best_lr_model = pipeline_model.stages[2].bestModel\n",
    "# print(best_lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beer type assessment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Feature Preparation\n",
    "features_cols = [\"Fermentation_Time\" ,\"Temperature\", \"pH_Level\" , \"Gravity\", \"Volume_Produced\", \"Brewhouse_Efficiency\"]\n",
    "\n",
    "# Dataset Preparation\n",
    "regr_fermloss_sample = df.alias(\"regr_fermloss_sample\")\n",
    "\n",
    "# Train-Test Split\n",
    "regr_fermloss_train, regr_fermloss_test = regr_fermloss_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 1. Vector Assembler:\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "\n",
    "# 2. Standard Scaler (If you decide to standardize):\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# 3. Regression Models:\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Loss_During_Fermentation\")\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Loss_During_Brewing\")\n",
    "\n",
    "# Grid Search for Hyperparameter Tuning\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Loss_During_Fermentation\", predictionCol=\"prediction\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Pipeline for Modeling Stages Only\n",
    "pipeline = Pipeline(stages=[assembler,scaler, crossval])\n",
    "\n",
    "# Fit the Pipelines:\n",
    "pipeline_model = pipeline.fit(regr_brewloss_train)\n",
    "\n",
    "# Evaluation: After getting predictions\n",
    "train_predictions = pipeline_model.transform(regr_fermloss_train)\n",
    "test_predictions = pipeline_model.transform(regr_fermloss_test)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train Root Mean Squared Error (mae): %f\" % train_rmse)\n",
    "print(\"Test Root Mean Squared Error (mae): %f\" % test_rmse)\n",
    "\n",
    "# # Access Best Model\n",
    "# best_lr_model = pipeline_model.stages[2].bestModel\n",
    "# print(best_lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_df = df.repartition('brew_date') \\\n",
    "              .orderBy('brew_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'total_sales'\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "brew_data = series_df.groupBy('brew_date').agg(F.mean(target_column).alias('avg_sales'))\n",
    "brew_data = brew_data.withColumn('brew_date', F.col('brew_date').cast(DateType()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_size = 7  # 7 day moving average\n",
    "windowSpec = Window.orderBy('brew_date').rowsBetween(-window_size + 1, 0)  # Look back window\n",
    "result_df = brew_data.withColumn('forecast', F.avg('avg_sales').over(windowSpec))\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
