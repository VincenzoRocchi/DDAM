{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and spark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set SPARK_HOME\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:/Spark/spark-3.5.0-bin-hadoop3\"\n",
    "\n",
    "# Set PYTHONPATH\n",
    "spark_home = os.environ.get(\"SPARK_HOME\", \"\")\n",
    "if spark_home:\n",
    "    python_path = os.path.join(spark_home, \"python\")\n",
    "    py4j_zip = os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9.7-src.zip\")\n",
    "    os.environ[\"PYTHONPATH\"] = f\"{python_path};{py4j_zip};{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Set PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\"\n",
    "print(os.environ.get('PYSPARK_PYTHON'))\n",
    "\n",
    "# Set PATH\n",
    "if spark_home:\n",
    "    os.environ[\"PATH\"] = f\"{python_path};{os.path.join(spark_home, 'bin')};{os.environ.get('PATH', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import gc\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2468acca520>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"spark\").config(\"spark.driver.memory\", \"24g\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "# spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(r\"c:\\Users\\Vincenzo\\PROJECTS\\DDAM_data\\brewery\\brewery_data_complete_extended.parquet\")\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batch_ID: long (nullable = true)\n",
      " |-- Brew_Date: string (nullable = true)\n",
      " |-- Beer_Style: string (nullable = true)\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Fermentation_Time: long (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- pH_Level: double (nullable = true)\n",
      " |-- Gravity: double (nullable = true)\n",
      " |-- Alcohol_Content: double (nullable = true)\n",
      " |-- Bitterness: long (nullable = true)\n",
      " |-- Color: long (nullable = true)\n",
      " |-- Ingredient_Ratio: string (nullable = true)\n",
      " |-- Volume_Produced: long (nullable = true)\n",
      " |-- Total_Sales: double (nullable = true)\n",
      " |-- Quality_Score: double (nullable = true)\n",
      " |-- Brewhouse_Efficiency: double (nullable = true)\n",
      " |-- Loss_During_Brewing: double (nullable = true)\n",
      " |-- Loss_During_Fermentation: double (nullable = true)\n",
      " |-- Loss_During_Bottling_Kegging: double (nullable = true)\n",
      "\n",
      "Dimension of the Dataframe is: (10000000, 20)\n",
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "|Batch_ID|          Brew_Date|Beer_Style| SKU|    Location|Fermentation_Time|       Temperature|          pH_Level|           Gravity|  Alcohol_Content|Bitterness|Color|Ingredient_Ratio|Volume_Produced|       Total_Sales|    Quality_Score|Brewhouse_Efficiency|Loss_During_Brewing|Loss_During_Fermentation|Loss_During_Bottling_Kegging|\n",
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "| 7870796|2020-01-01 00:00:19|Wheat Beer|Kegs|  Whitefield|               16|24.204250857069876|5.2898454476095615| 1.039504126730198|5.370842159553436|        20|    5|     1:0.32:0.16|           4666|2664.7593448382822| 8.57701633109399|   89.19588216376087| 4.1049876591878345|      3.2354851724654683|           4.663204448186049|\n",
      "| 9810411|2020-01-01 00:00:31|      Sour|Kegs|  Whitefield|               13|18.086762947259544| 5.275643382756193|1.0598189516987164|5.096053082797625|        36|   14|     1:0.39:0.24|            832|  9758.80106247132|7.420540752553908|    72.4809153900275|  2.676528095392112|      4.2461292104108574|            2.04435836917023|\n",
      "| 2623342|2020-01-01 00:00:40|Wheat Beer|Kegs| Malleswaram|               12|15.539332669116469|4.7780156232459765|  1.03747570954872|4.824737120959184|        30|   10|     1:0.35:0.16|           2115|11721.087016274963|8.451364886803127|   86.32214396020584|  3.299893625514981|       3.109440467362847|           3.033879837876281|\n",
      "| 8114651|2020-01-01 00:01:37|       Ale|Kegs| Rajajinagar|               17| 16.41848910394318| 5.345260585546188|1.0524314251694946|5.509243080797997|        48|   18|     1:0.35:0.15|           3173|12050.177463190275|9.671859404043175|   83.09494037181545|  2.136055116262562|       4.634254174098425|          1.4898890677148424|\n",
      "| 4579587|2020-01-01 00:01:43|     Stout|Cans|Marathahalli|               18| 19.14490765433852|  4.86185374113861|1.0542961149482333|5.133624684263243|        57|   13|     1:0.46:0.11|           4449|5515.0774647529615|7.895333676172065|   88.62583302052388|  4.491723843594972|      2.1833886016455497|          2.9906302188791485|\n",
      "+--------+-------------------+----------+----+------------+-----------------+------------------+------------------+------------------+-----------------+----------+-----+----------------+---------------+------------------+-----------------+--------------------+-------------------+------------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "print(f'Dimension of the Dataframe is: {(rows,cols)}')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and df assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch_ID</th>\n",
       "      <th>Fermentation_Time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>pH_Level</th>\n",
       "      <th>Gravity</th>\n",
       "      <th>Alcohol_Content</th>\n",
       "      <th>Bitterness</th>\n",
       "      <th>Color</th>\n",
       "      <th>Volume_Produced</th>\n",
       "      <th>Total_Sales</th>\n",
       "      <th>Quality_Score</th>\n",
       "      <th>Brewhouse_Efficiency</th>\n",
       "      <th>Loss_During_Brewing</th>\n",
       "      <th>Loss_During_Fermentation</th>\n",
       "      <th>Loss_During_Bottling_Kegging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "      <td>1000930.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999453.59</td>\n",
       "      <td>14.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.05</td>\n",
       "      <td>5.25</td>\n",
       "      <td>39.51</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2746.26</td>\n",
       "      <td>10504.09</td>\n",
       "      <td>8.00</td>\n",
       "      <td>79.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2889162.47</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>11.55</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1298.34</td>\n",
       "      <td>5486.58</td>\n",
       "      <td>1.15</td>\n",
       "      <td>5.78</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.03</td>\n",
       "      <td>4.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>500.00</td>\n",
       "      <td>1000.03</td>\n",
       "      <td>6.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2496035.25</td>\n",
       "      <td>12.00</td>\n",
       "      <td>17.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.87</td>\n",
       "      <td>29.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1622.00</td>\n",
       "      <td>5757.46</td>\n",
       "      <td>7.00</td>\n",
       "      <td>74.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999849.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.05</td>\n",
       "      <td>5.25</td>\n",
       "      <td>40.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2743.00</td>\n",
       "      <td>10504.97</td>\n",
       "      <td>8.00</td>\n",
       "      <td>79.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7504892.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>22.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.62</td>\n",
       "      <td>50.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3869.00</td>\n",
       "      <td>15259.37</td>\n",
       "      <td>9.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999997.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.08</td>\n",
       "      <td>6.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>4999.00</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Batch_ID  Fermentation_Time  Temperature    pH_Level     Gravity  \\\n",
       "count  1000930.00         1000930.00   1000930.00  1000930.00  1000930.00   \n",
       "mean   4999453.59              14.50        20.00        5.00        1.05   \n",
       "std    2889162.47               2.87         2.89        0.29        0.01   \n",
       "min          1.00              10.00        15.00        4.50        1.03   \n",
       "25%    2496035.25              12.00        17.50        4.75        1.04   \n",
       "50%    4999849.00              14.00        20.00        5.00        1.05   \n",
       "75%    7504892.00              17.00        22.50        5.25        1.07   \n",
       "max    9999997.00              19.00        25.00        5.50        1.08   \n",
       "\n",
       "       Alcohol_Content  Bitterness       Color  Volume_Produced  Total_Sales  \\\n",
       "count       1000930.00  1000930.00  1000930.00       1000930.00   1000930.00   \n",
       "mean              5.25       39.51       12.00          2746.26     10504.09   \n",
       "std               0.43       11.55        4.32          1298.34      5486.58   \n",
       "min               4.50       20.00        5.00           500.00      1000.03   \n",
       "25%               4.87       29.00        8.00          1622.00      5757.46   \n",
       "50%               5.25       40.00       12.00          2743.00     10504.97   \n",
       "75%               5.62       50.00       16.00          3869.00     15259.37   \n",
       "max               6.00       59.00       19.00          4999.00     20000.00   \n",
       "\n",
       "       Quality_Score  Brewhouse_Efficiency  Loss_During_Brewing  \\\n",
       "count     1000930.00            1000930.00           1000930.00   \n",
       "mean            8.00                 79.99                 3.00   \n",
       "std             1.15                  5.78                 1.16   \n",
       "min             6.00                 70.00                 1.00   \n",
       "25%             7.00                 74.99                 2.00   \n",
       "50%             8.00                 79.99                 3.00   \n",
       "75%             9.00                 85.00                 4.00   \n",
       "max            10.00                 90.00                 5.00   \n",
       "\n",
       "       Loss_During_Fermentation  Loss_During_Bottling_Kegging  \n",
       "count                1000930.00                    1000930.00  \n",
       "mean                       3.00                          3.00  \n",
       "std                        1.15                          1.15  \n",
       "min                        1.00                          1.00  \n",
       "25%                        2.00                          2.00  \n",
       "50%                        3.00                          3.00  \n",
       "75%                        4.00                          4.00  \n",
       "max                        5.00                          5.00  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stats = df.sample(False, 0.1, seed=10).toPandas().describe()\n",
    "rounded_describe = np.round(summary_stats, 2)\n",
    "rounded_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempted memory managment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# sample.unpersist()\n",
    "# del summary_stats\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Split the ratio_column into three parts based on the colon delimiter\n",
    "split_col = split(col(\"Ingredient_Ratio\"), \":\")\n",
    "\n",
    "# Create three separate columns for water, grains, and hops\n",
    "df = df.withColumn(\"water\", split_col.getItem(0).cast(\"double\"))\n",
    "df = df.withColumn(\"grains\", split_col.getItem(1).cast(\"double\"))\n",
    "df = df.withColumn(\"hops\", split_col.getItem(2).cast(\"double\"))\n",
    "df = df.drop(\"Ingredient_Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new column with the USD per liter ratio and brewing efficiency\n",
    "df = df.withColumn(\"USD_per_Liter\", F.col(\"Total_Sales\") / F.col(\"Volume_Produced\").cast(\"double\"))\n",
    "df = df.withColumn(\"Brewing_efficency\", F.col(\"Brewhouse_Efficiency\") / F.col(\"Volume_Produced\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['Batch_ID', 'Fermentation_Time', 'Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging', 'water', 'grains', 'hops', 'USD_per_Liter', 'Brewing_efficency']\n",
      "Categorical columns: ['Brew_Date', 'Beer_Style', 'SKU', 'Location']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Identify numerical and non-numerical columns\n",
    "numerical_cols = [col_name for col_name, data_type in df.dtypes if data_type != 'string']\n",
    "categorical_cols = [col_name for col_name, data_type in df.dtypes if data_type == 'string']\n",
    "\n",
    "print(f'Numerical columns: {numerical_cols}')\n",
    "print(f'Categorical columns: {categorical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_num_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    distinct_count = df.select(col).distinct().count()\n",
    "    distinct_num_counts[col] = distinct_count\n",
    "\n",
    "    print(f\"- {col} - {distinct_count} distinct values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_cat_counts = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    distinct_count = df.select(col).distinct().count()\n",
    "    distinct_cat_counts[col] = distinct_count\n",
    "\n",
    "    print(f\"- {col} - {distinct_count} distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batch_ID',\n",
       " 'Brew_Date',\n",
       " 'Beer_Style',\n",
       " 'SKU',\n",
       " 'Location',\n",
       " 'Fermentation_Time',\n",
       " 'Temperature',\n",
       " 'pH_Level',\n",
       " 'Gravity',\n",
       " 'Alcohol_Content',\n",
       " 'Bitterness',\n",
       " 'Color',\n",
       " 'Volume_Produced',\n",
       " 'Total_Sales',\n",
       " 'Quality_Score',\n",
       " 'Brewhouse_Efficiency',\n",
       " 'Loss_During_Brewing',\n",
       " 'Loss_During_Fermentation',\n",
       " 'Loss_During_Bottling_Kegging',\n",
       " 'water',\n",
       " 'grains',\n",
       " 'hops',\n",
       " 'USD_per_Liter',\n",
       " 'Brewing_efficency']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging', 'grains', 'hops', 'USD_per_Liter', 'Brewing_efficency']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "columns = df.columns\n",
    "filtered_columns = [col for col in columns if df.select(lower(col)).distinct().count() > 10]\n",
    "\n",
    "filtered_columns.remove('Brew_Date')\n",
    "filtered_columns.remove('Batch_ID')\n",
    "print(filtered_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewennes - Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|skewness(Temperature)|kurtosis(Temperature)|\n",
      "+---------------------+---------------------+\n",
      "|2.4518519193579075E-4|-1.2001740629835786  |\n",
      "+---------------------+---------------------+\n",
      "\n",
      "+--------------------+------------------+\n",
      "|skewness(pH_Level)  |kurtosis(pH_Level)|\n",
      "+--------------------+------------------+\n",
      "|5.810463584429394E-4|-1.199723583471544|\n",
      "+--------------------+------------------+\n",
      "\n",
      "+---------------------+------------------+\n",
      "|skewness(Gravity)    |kurtosis(Gravity) |\n",
      "+---------------------+------------------+\n",
      "|-3.541412548478481E-5|-1.200077666104571|\n",
      "+---------------------+------------------+\n",
      "\n",
      "+-------------------------+-------------------------+\n",
      "|skewness(Alcohol_Content)|kurtosis(Alcohol_Content)|\n",
      "+-------------------------+-------------------------+\n",
      "|6.555985166403937E-4     |-1.199645816573591       |\n",
      "+-------------------------+-------------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|skewness(Bitterness)|kurtosis(Bitterness)|\n",
      "+--------------------+--------------------+\n",
      "|9.561679966251545E-4|-1.2015700370434554 |\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+----------------------+-------------------+\n",
      "|skewness(Color)       |kurtosis(Color)    |\n",
      "+----------------------+-------------------+\n",
      "|-3.0236599657971125E-4|-1.2107253476374487|\n",
      "+----------------------+-------------------+\n",
      "\n",
      "+-------------------------+-------------------------+\n",
      "|skewness(Volume_Produced)|kurtosis(Volume_Produced)|\n",
      "+-------------------------+-------------------------+\n",
      "|3.7687881442967396E-4    |-1.2003506874038876      |\n",
      "+-------------------------+-------------------------+\n",
      "\n",
      "+---------------------+---------------------+\n",
      "|skewness(Total_Sales)|kurtosis(Total_Sales)|\n",
      "+---------------------+---------------------+\n",
      "|6.851469380232128E-4 |-1.200620319086929   |\n",
      "+---------------------+---------------------+\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|skewness(Quality_Score)|kurtosis(Quality_Score)|\n",
      "+-----------------------+-----------------------+\n",
      "|-1.9590489114601546E-4 |-1.1999727488704435    |\n",
      "+-----------------------+-----------------------+\n",
      "\n",
      "+------------------------------+------------------------------+\n",
      "|skewness(Brewhouse_Efficiency)|kurtosis(Brewhouse_Efficiency)|\n",
      "+------------------------------+------------------------------+\n",
      "|-2.371650365377202E-4         |-1.200296268250745            |\n",
      "+------------------------------+------------------------------+\n",
      "\n",
      "+-----------------------------+-----------------------------+\n",
      "|skewness(Loss_During_Brewing)|kurtosis(Loss_During_Brewing)|\n",
      "+-----------------------------+-----------------------------+\n",
      "|3.0444299902192403E-4        |-1.20001545775653            |\n",
      "+-----------------------------+-----------------------------+\n",
      "\n",
      "+----------------------------------+----------------------------------+\n",
      "|skewness(Loss_During_Fermentation)|kurtosis(Loss_During_Fermentation)|\n",
      "+----------------------------------+----------------------------------+\n",
      "|-6.016566170351203E-5             |-1.2002126665109873               |\n",
      "+----------------------------------+----------------------------------+\n",
      "\n",
      "+--------------------------------------+--------------------------------------+\n",
      "|skewness(Loss_During_Bottling_Kegging)|kurtosis(Loss_During_Bottling_Kegging)|\n",
      "+--------------------------------------+--------------------------------------+\n",
      "|-5.181435812255892E-4                 |-1.1998985286078632                   |\n",
      "+--------------------------------------+--------------------------------------+\n",
      "\n",
      "+---------------------+-------------------+\n",
      "|skewness(grains)     |kurtosis(grains)   |\n",
      "+---------------------+-------------------+\n",
      "|4.3866165542931994E-4|-1.1954053666766877|\n",
      "+---------------------+-------------------+\n",
      "\n",
      "+---------------------+-------------------+\n",
      "|skewness(hops)       |kurtosis(hops)     |\n",
      "+---------------------+-------------------+\n",
      "|-1.922905507251944E-4|-1.1884421941358834|\n",
      "+---------------------+-------------------+\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|skewness(USD_per_Liter)|kurtosis(USD_per_Liter)|\n",
      "+-----------------------+-----------------------+\n",
      "|2.3297553108478164     |6.757988439632339      |\n",
      "+-----------------------+-----------------------+\n",
      "\n",
      "+---------------------------+---------------------------+\n",
      "|skewness(Brewing_efficency)|kurtosis(Brewing_efficency)|\n",
      "+---------------------------+---------------------------+\n",
      "|1.8576320726824607         |3.1949381334801865         |\n",
      "+---------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "for col in filtered_columns:\n",
    "    df.select(skewness(col), kurtosis(col)).show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_kurt_col = [\"USD_per_Liter\",\"Brewing_efficency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's analyze the skewness and kurtosis values you've provided:\n",
    "\n",
    "Understanding Skewness\n",
    "\n",
    "    Positive Skewness: Indicates a tail stretching towards the right of the distribution (more observations with higher values).\n",
    "    Negative Skewness: Indicates a tail stretching towards the left of the distribution (more observations with lower values).\n",
    "    Zero (or Near-Zero) Skewness: Suggests a relatively symmetrical distribution.\n",
    "\n",
    "Understanding Kurtosis\n",
    "\n",
    "    Normal Distribution (Mesokurtic): Has a kurtosis around 0.\n",
    "    High Kurtosis (Leptokurtic): Indicates a sharper peak and heavier tails compared to a normal distribution (more outliers).\n",
    "    Low Kurtosis (Platykurtic): Indicates a flatter peak and lighter tails compared to a normal distribution (fewer outliers).\n",
    "\n",
    "Analyzing Your Data\n",
    "\n",
    "Here's a breakdown of what the skewness and kurtosis values suggest about your features:\n",
    "\n",
    "Generally:\n",
    "\n",
    "    Skewness: Most of your features have slightly positive skewness, meaning the data distribution might lean slightly towards higher values. However, the skewness values are close to zero for many features, suggesting relatively balanced distributions.\n",
    "    Kurtosis: All your features exhibit negative kurtosis, indicating that they might have flatter distributions than a standard normal distribution with fewer outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skeweness visualization - when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pdf = df.select('color').toPandas()\n",
    "\n",
    "# Create the count plot using seaborn\n",
    "import seaborn as sns\n",
    "sns.countplot(x='color', data=pdf, order=pdf['color'].value_counts().index) \n",
    "plt.title(\"Distribution of Color Feature\")\n",
    "plt.xticks(rotation=45)  # Rotate x labels if they are long\n",
    "plt.show()\n",
    "\n",
    "# Density Plot\n",
    "sns.kdeplot(pdf['color'])\n",
    "plt.title(\"Density Plot of Color Feature (Negative Skew)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile-Quantile plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more sparky approach\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def q_q_plot_pyspark(df, filtered_columns):\n",
    "    for col in filtered_columns:\n",
    "        quantiles = df.approxQuantile(col, [0.0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.0], 0.0)  \n",
    "\n",
    "        # Sample data for plotting on the theoretical normal distribution\n",
    "        theoretical_quantiles = F.array([F.lit(i) for i in quantiles]).cast(DoubleType())\n",
    "        std_norm_quantiles = F.expr(\"percentile_approx(array(0.0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.0), 0.5)\")\n",
    "\n",
    "        # QQ plot comparison\n",
    "        qqplot_data = df.select(col, theoretical_quantiles, std_norm_quantiles)\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas, this will likely be more manageable\n",
    "        pandas_df = qqplot_data.toPandas()\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure()\n",
    "        plt.scatter(pandas_df[col], pandas_df['std_norm_quantiles'])\n",
    "        plt.xlabel(f\"Sample Quantiles ({col})\")\n",
    "        plt.ylabel(\"Theoretical Normal Quantiles\")\n",
    "        plt.title(f\"QQ Plot for Column: {col}\")\n",
    "        plt.show()\n",
    "\n",
    "q_q_plot_pyspark(df, filtered_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def q_q_plot(df, filtered_columns):\n",
    "    for col in filtered_columns:\n",
    "        fig, ax = plt.subplots()\n",
    "        sm.qqplot(df.select(col).toPandas()[col], line='s', ax=ax)\n",
    "\n",
    "        # Add title with column name\n",
    "        ax.set_title(f\"QQ Plot for Column: {col}\") \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "q_q_plot(df, filtered_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Q-Q Plot:\n",
    "\n",
    "    Normal Distribution: If the data points closely follow the diagonal line, the distribution is likely close to normal.\n",
    "    Negative Skew (Long Left Tail): If the points curve below the line on the left side, it indicates a negative skew.\n",
    "    Positive Skew (Long Right Tail): If the points curve above the line on the right side, it indicates a positive skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not needed misinterpreted results from skewenss due to truncated show df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F.log1p() function is a PySpark function that calculates the natural logarithm of a column value plus one. It is commonly used to transform skewed or heavily right-skewed data to a more normalized distribution. By adding one to each value before taking the logarithm, it ensures that the function can handle zero or negative values without throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_skew_col = [\"pH_Level\", \"Gravity\", \"Alcohol_Content\"]\n",
    "\n",
    "transformed_df = df.alias(\"transformed_df\")\n",
    "\n",
    "transformed_df = transformed_df.withColumn(\"pH_Level\", F.log1p(\"pH_Level\"))\n",
    "transformed_df = transformed_df.withColumn(\"Gravity\", F.log1p(\"Gravity\"))\n",
    "transformed_df = transformed_df.withColumn(\"Alcohol_Content\", F.log1p(\"Alcohol_Content\"))\n",
    "\n",
    "df.select(high_skew_col).show(5)\n",
    "transformed_df.select(high_skew_col).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Ingredient_Ratio', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.0\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 0\n",
    "- Number of Outliers based on Gravity: 0\n",
    "- Number of Outliers based on Alcohol_Content: 0\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n",
    "- Number of Outliers based on grains: 0\n",
    "- Number of Outliers based on hops: 0\n",
    "- Number of Outliers based on USD_per_Liter: 250211\n",
    "- Number of Outliers based on Brewing_efficency: 247848\n",
    "\n",
    "##### Threshold 2.0 / 2.5 - (same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, abs, sqrt, avg, pow, when\n",
    "\n",
    "def detect_outliers_zscore(df, feature_col, threshold=3.0):\n",
    "    mean_val = df.select(avg(df[feature_col])).collect()[0][0] \n",
    "    std_dev = df.select(sqrt(avg(pow(df[feature_col] - mean_val, 2)))).collect()[0][0]  \n",
    "\n",
    "    return df.withColumn(\"outlier_\" + feature_col, \n",
    "                          when(abs((df[feature_col] - mean_val) / std_dev) > threshold, 1).otherwise(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers based on Temperature: 0\n",
      "Number of Outliers based on pH_Level: 0\n",
      "Number of Outliers based on Gravity: 0\n",
      "Number of Outliers based on Alcohol_Content: 0\n",
      "Number of Outliers based on Bitterness: 0\n",
      "Number of Outliers based on Color: 0\n",
      "Number of Outliers based on Volume_Produced: 0\n",
      "Number of Outliers based on Total_Sales: 0\n",
      "Number of Outliers based on Quality_Score: 0\n",
      "Number of Outliers based on Brewhouse_Efficiency: 0\n",
      "Number of Outliers based on Loss_During_Brewing: 0\n",
      "Number of Outliers based on Loss_During_Fermentation: 0\n",
      "Number of Outliers based on Loss_During_Bottling_Kegging: 0\n",
      "Number of Outliers based on grains: 0\n",
      "Number of Outliers based on hops: 0\n",
      "Number of Outliers based on USD_per_Liter: 250211\n",
      "Number of Outliers based on Brewing_efficency: 247848\n"
     ]
    }
   ],
   "source": [
    "for col in filtered_columns:\n",
    "    outlier = detect_outliers_zscore(df, col)\n",
    "    outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "    # Filtering directly on the 'outlier' DataFrame \n",
    "    outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "    print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier = detect_outliers_zscore(df, \"Bitterness\")\n",
    "# outlier_temp = outlier.filter(col(\"outlier_Bitterness\") == 1).count()\n",
    "# print(f\"Number of Outliers based on Bitterness: {outlier_temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  modified z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Ingredient_Ratio', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.5\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 0\n",
    "- Number of Outliers based on Gravity: 0\n",
    "- Number of Outliers based on Alcohol_Content: 0\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n",
    "- Number of Outliers based on grains: 0\n",
    "- Number of Outliers based on hops: 0\n",
    "- Number of Outliers based on USD_per_Liter: 428330\n",
    "- Number of Outliers based on Brewing_efficency: 1397253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, abs, median, sqrt, avg, pow\n",
    "\n",
    "# def detect_outliers_modified_zscore(df, feature_col, threshold=3.5):\n",
    "#     median_val = df.select(df[feature_col]).collect()[0][0] \n",
    "#     mad = df.select(median(abs(df[feature_col] - median_val))).collect()[0][0]\n",
    "\n",
    "#     return df.withColumn(\"outlier_\" + feature_col, \n",
    "#                           when(abs(0.6745 * (df[feature_col] - median_val) / mad) > threshold, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in filtered_columns:\n",
    "#     outlier = detect_outliers_modified_zscore(df, col)\n",
    "#     outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "#     # Filtering directly on the 'outlier' DataFrame \n",
    "#     outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "#     print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  modified z-score - adaptive threshold - i doubt the statistical right of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Temperature', 'pH_Level', 'Gravity', 'Alcohol_Content', 'Bitterness', 'Color', 'Volume_Produced', 'Total_Sales', 'Quality_Score', 'Brewhouse_Efficiency', 'Loss_During_Brewing', 'Loss_During_Fermentation', 'Loss_During_Bottling_Kegging']\n",
    "\n",
    "##### threshold 3.5\n",
    "- Number of Outliers based on Temperature: 0\n",
    "- Number of Outliers based on pH_Level: 3556568\n",
    "- Number of Outliers based on Gravity: 9535830\n",
    "- Number of Outliers based on Alcohol_Content: 189356\n",
    "- Number of Outliers based on Bitterness: 0\n",
    "- Number of Outliers based on Color: 0\n",
    "- Number of Outliers based on Volume_Produced: 0\n",
    "- Number of Outliers based on Total_Sales: 0\n",
    "- Number of Outliers based on Quality_Score: 0\n",
    "- Number of Outliers based on Brewhouse_Efficiency: 0\n",
    "- Number of Outliers based on Loss_During_Brewing: 0\n",
    "- Number of Outliers based on Loss_During_Fermentation: 0\n",
    "- Number of Outliers based on Loss_During_Bottling_Kegging: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_modified_zscore_adaptive(df, feature_col, threshold_factor=2.0):\n",
    "\n",
    "    median_val = df.select(df[feature_col]).collect()[0][0] \n",
    "    mad = df.select(median(abs(df[feature_col] - median_val))).collect()[0][0]\n",
    "\n",
    "    std_dev = np.std(df.select(feature_col).toPandas()[feature_col])  # Assuming numerical column \n",
    "    adaptive_threshold =  threshold_factor * std_dev\n",
    "\n",
    "    return df.withColumn(\"outlier_\" + feature_col, \n",
    "                          when(abs(0.6745 * (df[feature_col] - median_val) / mad) > adaptive_threshold, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_columns.remove(\"Ingredient_Ratio\")\n",
    "\n",
    "# for col in filtered_columns:\n",
    "#     outlier = detect_outliers_modified_zscore_adaptive(df, col)\n",
    "#     outlier_col_name = f\"outlier_{col}\"\n",
    "\n",
    "#     # Filtering directly on the 'outlier' DataFrame \n",
    "#     outlier_count = outlier.filter(outlier[outlier_col_name] == 1).count()\n",
    "#     print(f\"Number of Outliers based on {col}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-values continous variables analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, dense_rank, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def ordinal_encode_pyspark(df, input_col, output_col):\n",
    "    \"\"\" Ordinal encodes a column within a PySpark DataFrame\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input Spark DataFrame.\n",
    "        input_col (str): Name of the column to be encoded. Defaults to \"Bitterness\".\n",
    "        output_col (str): Name for the new ordinal encoded column. Defaults to \"Bitterness_ordinal\".\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The Spark DataFrame with the ordinal encoded column.\n",
    "    \"\"\"\n",
    "\n",
    "    window = Window.orderBy(input_col) \n",
    "    return df.withColumn(output_col, dense_rank().over(window)) \\\n",
    "             .withColumn(output_col, when(col(input_col).isNull(), 0).otherwise(col(output_col))) \\\n",
    "             #.drop(input_col)\n",
    "             \n",
    "def check_distinct_values(df, feature_col):\n",
    "    \"\"\"  \n",
    "    Prints the distinct values of a feature in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The PySpark DataFrame.\n",
    "        feature_col (str): The name of the feature column.\n",
    "    \"\"\"\n",
    "\n",
    "    distinct_values = df.select(feature_col).distinct().collect()\n",
    "\n",
    "    print(f\"Distinct values in '{feature_col}':\")\n",
    "    for row in distinct_values:\n",
    "        print(row[feature_col]) \n",
    "\n",
    "    # functions\n",
    "\n",
    "def one_hot_encode_column(df, input_col, output_col_prefix=\"encoded\"):\n",
    "    \"\"\" One-hot encodes a specified categorical column, drops intermediary columns,\n",
    "        and allows for specifying the prefix of the output columns.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input DataFrame.\n",
    "        input_col (str): The name of the column containing categorical data.\n",
    "        output_col_prefix (str): Prefix for the generated one-hot encoded column names.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame with the one-hot encoded column(s) and \n",
    "                               intermediary columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    indexer = StringIndexer(inputCol=input_col, outputCol=input_col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCol=input_col + \"_index\", outputCol=output_col_prefix + input_col + \"features\")\n",
    "    assembler = VectorAssembler(inputCols=[input_col + \"_index\"], outputCol=\"features\") \n",
    "\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, encoder])\n",
    "    transformed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [input_col + \"_index\", \"features\"]\n",
    "    return transformed_df.drop(*drop_cols)  # Dynamic column dropping\n",
    "\n",
    "def discretize_and_encode(df, input_col=\"fermentation_time\"):\n",
    "    \"\"\" Discretizes a feature into bins, one-hot encodes the bins, and preserves the original column\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input DataFrame\n",
    "        input_col (str): Name of the column to be discretized and encoded\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame:  DataFrame with new discretization and encoded columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(input_col + \"_bin\", \n",
    "                       when((col(input_col) >= 10) & (col(input_col) <= 13), \"Short\")\n",
    "                       .when((col(input_col) >= 14) & (col(input_col) <= 16), \"Medium\")\n",
    "                       .otherwise(\"Long\"))\n",
    "    \n",
    "    return one_hot_encode_column(df, input_col=input_col + \"_bin\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sample = df.select(\"Color\").sample(fraction=0.1, seed=42).alias(\"color_sample\")\n",
    "color_frequency = color_sample.groupBy(\"Color\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=color_frequency, x=\"Color\", y=\"count\", order=color_frequency.sort_values('count', ascending=False)[\"Color\"])\n",
    "plt.title('Frequency of Color in the Dataset')\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ordinal_encode_pyspark(sample, \"Color\", \"Color_ordinal\")\n",
    "# sample.show(5)\n",
    "\n",
    "# summary_stats = sample.select(\"Color_ordinal\").toPandas().describe()\n",
    "# rounded_describe = np.round(summary_stats, 2)\n",
    "# rounded_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_frequency = sample.groupBy(\"Color_ordinal\").count().toPandas()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(data=color_frequency, x=\"Color_ordinal\", y=\"count\", order=color_frequency.sort_values('Color_ordinal', ascending=True)[\"Color_ordinal\"])\n",
    "# plt.title('Frequency of Color in the Dataset')\n",
    "# plt.xlabel('Color')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bitterness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitterness_sample = df.select(\"Bitterness\").sample(fraction=0.1, seed=42).alias(\"bitterness_sample\")\n",
    "Bitterness_frequency = bitterness_sample.groupBy(\"Bitterness\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=Bitterness_frequency, x=\"Bitterness\", y=\"count\", order=Bitterness_frequency.sort_values('Bitterness', ascending=True)[\"Bitterness\"])\n",
    "plt.title('Frequency of Bitterness in the Dataset')\n",
    "plt.xlabel('Bitterness')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ordinal_encode_pyspark(sample, \"Bitterness\", \"Bitterness_ordinal\") \n",
    "# sample.show(5)\n",
    "\n",
    "# summary_stats = sample.select(\"Bitterness_ordinal\").toPandas().describe()\n",
    "# rounded_describe = np.round(summary_stats, 2)\n",
    "# rounded_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitterness_ordinal_frequency = sample.groupBy(\"Bitterness_ordinal\").count().toPandas()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(data=Bitterness_ordinal_frequency, x=\"Bitterness_ordinal\", y=\"count\", order=Bitterness_ordinal_frequency.sort_values('Bitterness_ordinal', ascending=True)[\"Bitterness_ordinal\"])\n",
    "# plt.title('Frequency of Bitterness_ordinal in the Dataset')\n",
    "# plt.xlabel('Bitterness_ordinal')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_sample = df.select(\"grains\").sample(fraction=0.1, seed=42).alias(\"grain_sample\")\n",
    "grains_frequency = grain_sample.groupBy(\"grains\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=grains_frequency, x=\"grains\", y=\"count\", order=grains_frequency.sort_values('grains', ascending=True)[\"grains\"])\n",
    "plt.title('Frequency of Styles')\n",
    "plt.xlabel('Beer_Style')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hops_sample = df.select(\"hops\").sample(fraction=0.1, seed=42).alias(\"hops_sample\")\n",
    "hops_frequency = hops_sample.groupBy(\"hops\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=hops_frequency, x=\"hops\", y=\"count\", order=hops_frequency.sort_values('hops', ascending=True)[\"hops\"])\n",
    "plt.title('Frequency of Hops')\n",
    "plt.xlabel('Hops')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fermentation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretize and encode is hardcoded for fermentation time (!!)\n",
    "\n",
    "df = discretize_and_encode(df)\n",
    "df.select(\"encodedfermentation_time_binfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedfermentation_time_binfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beer style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|encodedBeer_Stylefeatures|\n",
      "+-------------------------+\n",
      "|                (7,[],[])|\n",
      "|            (7,[2],[1.0])|\n",
      "|                (7,[],[])|\n",
      "|            (7,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"Beer_Style\")\n",
    "df.select(\"encodedBeer_Stylefeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedBeer_Stylefeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|encodedSKUfeatures|\n",
      "+------------------+\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|         (3,[],[])|\n",
      "|     (3,[2],[1.0])|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"SKU\")\n",
    "df.select(\"encodedSKUfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedSKUfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|encodedLocationfeatures|\n",
      "+-----------------------+\n",
      "|              (9,[],[])|\n",
      "|              (9,[],[])|\n",
      "|          (9,[7],[1.0])|\n",
      "|          (9,[4],[1.0])|\n",
      "|          (9,[3],[1.0])|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = one_hot_encode_column(df, \"Location\")\n",
    "df.select(\"encodedLocationfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"encodedLocationfeatures\").sample(fraction=0.1, seed= 42).toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month , to_date,  col\n",
    "\n",
    "plot_sample = df.alias(\"plot_sample\")\n",
    "\n",
    "plot_sample = plot_sample.withColumn(\"Year\", year(\"Brew_Date\")).withColumn(\"Month\", month(\"Brew_Date\"))\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "print(f'We have data in a range from {plot_sample.select(F.min(\"year\")).collect()[0][0]} to {plot_sample.select(F.max(\"year\")).collect()[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=plot_sample.select(\"USD_per_Liter\").toPandas(), x=\"USD_per_Liter\", kde=True)\n",
    "\n",
    "plt.title(f'Histogram for USD_per_Liter')\n",
    "plt.xlabel(\"USD_per_Liter\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "key_mode_count = plot_sample.groupBy('Color', 'Beer_Style').count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size to 10 inches wide and 5 inches tall\n",
    "sns.barplot(x=\"Color\", y=\"count\", data=key_mode_count, hue='Beer_Style')\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "#that is conceptually wrong there cant be the same distribution of type of beers for each color basically impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('SKU', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for SKU in pandas_df['SKU'].unique():\n",
    "   data = pandas_df[pandas_df['SKU'] == SKU]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=SKU)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by SKU\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('Beer_Style', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for Beer_Style in pandas_df['Beer_Style'].unique():\n",
    "   data = pandas_df[pandas_df['Beer_Style'] == Beer_Style]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=Beer_Style)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by Beer_Style\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date,mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('Location', 'Brew_Date') \\\n",
    "                           .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for location in pandas_df['Location'].unique():\n",
    "   data = pandas_df[pandas_df['Location'] == location]\n",
    "   plt.plot(data['Brew_Date'], data['Total_Sales'], label=location)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Sales Trends by Location\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Calculation - Aggregate total sales by date \n",
    "overall_sales_trend = plot_sample.groupBy('Brew_Date') \\\n",
    "                        .agg(mean('Total_Sales').alias('Total_Sales')) \\\n",
    "                        .orderBy('Brew_Date')\n",
    "\n",
    "# Convert to pandas and plot\n",
    "pandas_df = overall_sales_trend.toPandas()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pandas_df['Brew_Date'], pandas_df['Total_Sales'])\n",
    "\n",
    "# Customize plot elements \n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Overall Sales Trend\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date, mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Calculation - Aggregate total sales by date \n",
    "overall_sales_trend = plot_sample.groupBy('Brew_Date') \\\n",
    "                        .agg(mean('Temperature').alias('Temperature')) \\\n",
    "                        .orderBy('Brew_Date')\n",
    "\n",
    "# Convert to pandas and plot\n",
    "pandas_df = overall_sales_trend.toPandas()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(pandas_df['Brew_Date'], pandas_df['Temperature'])\n",
    "\n",
    "# Customize plot elements \n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.title(\"Overall Temperature Trend\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, to_date,mean\n",
    "from pyspark.sql.types import DateType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plot_sample = plot_sample.withColumn('Brew_Date', to_date(col('Brew_Date'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "sales_by_location_date = plot_sample.groupBy('SKU', 'Brew_Date') \\\n",
    "                           .agg(mean('Fermentation_Time').alias('Fermentation_Time')) \\\n",
    "                           .orderBy('Brew_Date')\n",
    "\n",
    "# Convert Spark DataFrame to pandas for plotting\n",
    "pandas_df = sales_by_location_date.toPandas()\n",
    "\n",
    "# Configure plot size\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Iterate over locations to create separate trend lines\n",
    "for SKU in pandas_df['SKU'].unique():\n",
    "   data = pandas_df[pandas_df['SKU'] == SKU]\n",
    "   plt.plot(data['Brew_Date'], data['Fermentation_Time'], label=SKU)\n",
    "\n",
    "# Customize plot elements (same as before)\n",
    "plt.xlabel(\"Brew Date\")\n",
    "plt.ylabel(\"Fermentation_Time \")\n",
    "plt.title(\"Fermentation_Time Trends by SKU\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample = plot_sample.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### losses against other\n",
    "\n",
    "- brewhouse efficency (no patterns \\ uniform)\n",
    "- usd per liter (no patterns \\ uniform)\n",
    "- year (no patterns \\ uniform)\n",
    "- month (no patterns \\ uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,4))  # Set the figure size to 10 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Brewing\", y=\"Brewhouse_Efficiency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))  # Set the figure size to 10 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Fermentation\", y=\"Year\")\n",
    "plt.yticks([2020,2021,2022,2023])\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size to 12 inches wide and 6 inches tall\n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Bottling_Kegging\", y=\"Month\")\n",
    "plt.yticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quality scores against other\n",
    "\n",
    "- Color (no patterns \\ uniform)\n",
    "- Alcohol_Content (no patterns \\ uniform)\n",
    "- Bitterness (no patterns \\ uniform)\n",
    "- volume produced (no patterns \\ uniform)\n",
    "- total sales (no patterns \\ uniform)\n",
    "- brewhouse efficency (no patterns \\ uniform)\n",
    "- usd per liter (no patterns \\ uniform)\n",
    "\n",
    "\n",
    "add to the quality score plotting all the features that are strictly related to the fermentation process also aginst losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"USD_per_Liter\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Fermentation_Time\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Temperature\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"pH_Level\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Gravity\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Alcohol_Content\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Bitterness\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Volume_Produced\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Brewing\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Fermentation\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"Loss_During_Bottling_Kegging\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"grains\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "sns.scatterplot(data=plot_sample.toPandas(), x=\"hops\", y=\"Quality_Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample = df.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample = corr_sample.drop(\"water\", \"Batch_ID\",\"encodedBeer_Stylefeatures\", \"encodedSKUfeatures\", \"encodedLocationfeatures\")\n",
    "corr_sample = corr_sample.drop(\"Brew_Date\", \"encodedfermentation_time_binfeatures\")\n",
    "corr_sample = corr_sample.drop(\"Beer_Style\", \"SKU\", \"Location\", \"fermentation_time_bin\")\n",
    "corr_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.corr(method='pearson')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.corr(method='spearman')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_sample.toPandas().corr(method='kendall')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# features = corr_sample.schema.names\n",
    "# vectorassembler = VectorAssembler(inputCols = features, outputCol= 'assemblerfeatures')\n",
    "# output_dataset = vectorassembler.transform(corr_sample)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.stat import Correlation\n",
    "\n",
    "# pearsonCorr = Correlation.corr(output_dataset, 'assemblerfeatures', 'pearson').collect()[0][0]\n",
    "\n",
    "# #trasformo la DenseMatrix in un array numpy\n",
    "# correlation_array = pearsonCorr.toArray() #ritorna un numpy.ndarray\n",
    "\n",
    "# correlationDF = pd.DataFrame(\n",
    "#     correlation_array,\n",
    "#     index = features,\n",
    "#     columns = features\n",
    "# )\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(15,12)})\n",
    "# sns.heatmap(correlationDF, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_df = df.repartition('brew_date') \\\n",
    "              .orderBy('brew_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'total_sales'\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "brew_data = series_df.groupBy('brew_date').agg(F.mean(target_column).alias('avg_sales'))\n",
    "brew_data = brew_data.withColumn('brew_date', F.col('brew_date').cast(DateType()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_size = 7  # 7 day moving average\n",
    "windowSpec = Window.orderBy('brew_date').rowsBetween(-window_size + 1, 0)  # Look back window\n",
    "result_df = brew_data.withColumn('forecast', F.avg('avg_sales').over(windowSpec))\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
